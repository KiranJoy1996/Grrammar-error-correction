{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSbFUJW3Rq66"
   },
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "xtYwh6xLP-rR",
    "outputId": "72c5d2d9-c7eb-455f-b69b-08ea57c8fde9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.8.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Conv2D, Flatten , Input , Conv1D , Concatenate , MaxPooling1D , Dropout , Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import datetime\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import  f1_score , roc_auc_score\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-B9V6UqiIsOp",
    "outputId": "021819e4-9ec3-4846-e1f7-c601eb35784f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "Q75mMFTpQGn2",
    "outputId": "bb10cc38-19fb-4012-f5c3-111a65ca6cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498362, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1d3b8cdb-cd03-4e26-baf5-73865368da1b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And he took in my favorite subjects like soccer .</td>\n",
       "      <td>And he took in my favorite subject like soccer .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actually , he was the one who let me know abou...</td>\n",
       "      <td>Actually , who let me know about Lang - 8 was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>His Kanji ability is much better than mine .</td>\n",
       "      <td>His Kanji 's ability is much better than me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We 've known each other for only half a year ,...</td>\n",
       "      <td>We 've known each other for only half a year ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I heard a sentence last night when I was watch...</td>\n",
       "      <td>I heard a sentence last night when I watched TV .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d3b8cdb-cd03-4e26-baf5-73865368da1b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1d3b8cdb-cd03-4e26-baf5-73865368da1b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1d3b8cdb-cd03-4e26-baf5-73865368da1b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                             correct  \\\n",
       "0  And he took in my favorite subjects like soccer .   \n",
       "1  Actually , he was the one who let me know abou...   \n",
       "2       His Kanji ability is much better than mine .   \n",
       "3  We 've known each other for only half a year ,...   \n",
       "4  I heard a sentence last night when I was watch...   \n",
       "\n",
       "                                           incorrect  \n",
       "0   And he took in my favorite subject like soccer .  \n",
       "1  Actually , who let me know about Lang - 8 was ...  \n",
       "2      His Kanji 's ability is much better than me .  \n",
       "3  We 've known each other for only half a year ,...  \n",
       "4  I heard a sentence last night when I watched TV .  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0OjTA4DI1_1",
    "outputId": "9330b57d-862a-43a5-80de-b6ba42690514"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckeck if any missing value is present\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "369xvhQgI1_5",
    "outputId": "a74615a4-a47a-4530-ce98-d02c3208d6cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498360, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iV93hk0YI1_7",
    "outputId": "082d147b-6e5c-4e0a-dd51-abdee2d9fcc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496339, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IenJyc8lI1_8"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    takes string as input and\n",
    "    removes characters inside (),{},[] and <>\n",
    "    removes characters like -+@#^/|*(){}$~`\n",
    "    we not not removing ,.!-:;\"' as these characters are present in english language \n",
    "    \"\"\"\n",
    "    text = re.sub('<.*>', '', text)\n",
    "    text = re.sub('\\(.*\\)', '', text)\n",
    "    text = re.sub('\\[.*\\]', '', text)\n",
    "    text = re.sub('{.*}', '', text)\n",
    "    text = re.sub(\"[-+@#^/|*(){}$~`<>=_]\",\"\",text)\n",
    "    text = text.replace(\"\\\\\",\"\")\n",
    "    text = re.sub(\"\\[\",\"\",text)\n",
    "    text = re.sub(\"\\]\",\"\",text)\n",
    "    text = re.sub(\"[0-9]\",\"\",text)\n",
    "    return text\n",
    "\n",
    "data[\"correct\"] = data[\"correct\"].apply(clean)\n",
    "data[\"incorrect\"] = data[\"incorrect\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A74MvZgTI1_-"
   },
   "outputs": [],
   "source": [
    "def percentile(low,high,step,list_temp):\n",
    "    \"\"\"\n",
    "    this function takes low, high, step size as input and prints percentiles accordingly\n",
    "    \"\"\"\n",
    "    for i in np.arange(low,high,step):\n",
    "        print(i,\"percentile is \",np.percentile(list_temp ,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "od19dYf7I1__",
    "outputId": "ff3dfe03-b462-4f83-c63f-7d99b96cfbc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496339"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sen_to_char(sen):\n",
    "    return len([i for i in sen])\n",
    "\n",
    "corr_length = data[\"correct\"].apply(sen_to_char)\n",
    "corr_length = list(corr_length)\n",
    "len(corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEpALxp7I2AA",
    "outputId": "79ce339f-7be3-4d0a-c9bf-0982ae475bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  27.0\n",
      "20 percentile is  35.0\n",
      "30 percentile is  42.0\n",
      "40 percentile is  48.0\n",
      "50 percentile is  55.0\n",
      "60 percentile is  63.0\n",
      "70 percentile is  73.0\n",
      "80 percentile is  86.0\n",
      "90 percentile is  108.0\n",
      "100 percentile is  2622.0\n",
      "***************************************************************\n",
      "90 percentile is  108.0\n",
      "91 percentile is  112.0\n",
      "92 percentile is  116.0\n",
      "93 percentile is  120.0\n",
      "94 percentile is  125.0\n",
      "95 percentile is  131.0\n",
      "96 percentile is  139.0\n",
      "97 percentile is  149.0\n",
      "98 percentile is  163.0\n",
      "99 percentile is  188.0\n",
      "100 percentile is  2622.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,corr_length)\n",
    "print(\"***************************************************************\")\n",
    "percentile(90,101,1,corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-sZ1CEYI2AB",
    "outputId": "1cc3a98f-9c15-41b7-ad36-663e690b1aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446785, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have correct sentence of length more than 108\n",
    "index = []\n",
    "for i in range(len(corr_length)):\n",
    "    if corr_length[i] > 108:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyJz6MQLI2AD",
    "outputId": "847c41c8-f26e-4b24-9847-67be6bc814a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446785"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorr_length = data[\"incorrect\"].apply(sen_to_char)\n",
    "incorr_length = list(incorr_length)\n",
    "len(incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCabVH9eI2AF",
    "outputId": "8e420cc4-2220-4f96-c3d0-6c003f13324e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  25.0\n",
      "20 percentile is  32.0\n",
      "30 percentile is  38.0\n",
      "40 percentile is  44.0\n",
      "50 percentile is  50.0\n",
      "60 percentile is  57.0\n",
      "70 percentile is  64.0\n",
      "80 percentile is  73.0\n",
      "90 percentile is  85.0\n",
      "100 percentile is  611.0\n",
      "***********************************************************\n",
      "90 percentile is  85.0\n",
      "91 percentile is  87.0\n",
      "92 percentile is  89.0\n",
      "93 percentile is  91.0\n",
      "94 percentile is  92.0\n",
      "95 percentile is  95.0\n",
      "96 percentile is  97.0\n",
      "97 percentile is  99.0\n",
      "98 percentile is  103.0\n",
      "99 percentile is  107.0\n",
      "100 percentile is  611.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,incorr_length)\n",
    "print(\"***********************************************************\")\n",
    "percentile(90,101,1,incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4O7aEaB9I2AF",
    "outputId": "f76437ca-c35f-461e-805a-10ed44de0830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443397, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have incorrect sentence of length more than 108\n",
    "index = []\n",
    "for i in range(len(incorr_length)):\n",
    "    if incorr_length[i] > 108:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cgPxta2jI2AG"
   },
   "outputs": [],
   "source": [
    "corr_length = data[\"correct\"].str.split().apply(len)\n",
    "corr_length = list(corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zd1LHGECI2AH",
    "outputId": "79fdc314-3213-44da-8ba3-0e63bbe6f521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  6.0\n",
      "20 percentile is  8.0\n",
      "30 percentile is  9.0\n",
      "40 percentile is  10.0\n",
      "50 percentile is  11.0\n",
      "60 percentile is  13.0\n",
      "70 percentile is  14.0\n",
      "80 percentile is  16.0\n",
      "90 percentile is  18.0\n",
      "100 percentile is  47.0\n",
      "***************************************************************\n",
      "90 percentile is  18.0\n",
      "91 percentile is  19.0\n",
      "92 percentile is  19.0\n",
      "93 percentile is  19.0\n",
      "94 percentile is  20.0\n",
      "95 percentile is  20.0\n",
      "96 percentile is  21.0\n",
      "97 percentile is  21.0\n",
      "98 percentile is  22.0\n",
      "99 percentile is  23.0\n",
      "100 percentile is  47.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,corr_length)\n",
    "print(\"***************************************************************\")\n",
    "percentile(90,101,1,corr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHtP99pDI2AI",
    "outputId": "348c1792-87e4-4eb7-b6a4-d0045ce764e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441931, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have correct sentence of length more than 24\n",
    "index = []\n",
    "for i in range(len(corr_length)):\n",
    "    if corr_length[i] > 24:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "C4YdDZV8I2AJ"
   },
   "outputs": [],
   "source": [
    "incorr_length = data[\"incorrect\"].str.split().apply(len)\n",
    "incorr_length = list(incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzGpEUWeI2AK",
    "outputId": "d727186d-4c0e-43dd-e5be-df924d70c64c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percentile is  0.0\n",
      "10 percentile is  6.0\n",
      "20 percentile is  7.0\n",
      "30 percentile is  8.0\n",
      "40 percentile is  10.0\n",
      "50 percentile is  11.0\n",
      "60 percentile is  12.0\n",
      "70 percentile is  14.0\n",
      "80 percentile is  15.0\n",
      "90 percentile is  18.0\n",
      "100 percentile is  31.0\n",
      "***************************************************************\n",
      "90 percentile is  18.0\n",
      "91 percentile is  18.0\n",
      "92 percentile is  18.0\n",
      "93 percentile is  19.0\n",
      "94 percentile is  19.0\n",
      "95 percentile is  20.0\n",
      "96 percentile is  20.0\n",
      "97 percentile is  21.0\n",
      "98 percentile is  21.0\n",
      "99 percentile is  22.0\n",
      "100 percentile is  31.0\n"
     ]
    }
   ],
   "source": [
    "percentile(0,101,10,incorr_length)\n",
    "print(\"***************************************************************\")\n",
    "percentile(90,101,1,incorr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urDbnhEWI2AK",
    "outputId": "f9bfa004-af66-4aeb-9c5c-012f2635dfd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441842, 2)\n"
     ]
    }
   ],
   "source": [
    "# removing those data points which have incorrect sentence of length more than 25\n",
    "index = []\n",
    "for i in range(len(incorr_length)):\n",
    "    if incorr_length[i] > 25:\n",
    "        index.append(i)\n",
    "        \n",
    "data.drop(index,inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V0t7mGktI2AL"
   },
   "outputs": [],
   "source": [
    "data[\"correct\"] = data[\"correct\"].astype(str)\n",
    "data[\"incorrect\"] = data[\"incorrect\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP5rbIrmQpVd",
    "outputId": "cba90065-46a3-49df-c431-9ae02eb26ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.correct) , data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBzZDw87QmL1",
    "outputId": "70fd83c2-6caf-4f3a-b5f5-2bfe1987baa9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441842/441842 [00:00<00:00, 2244485.03it/s]\n",
      "100%|██████████| 441842/441842 [00:00<00:00, 2424150.82it/s]\n",
      "100%|██████████| 441842/441842 [00:00<00:00, 2432180.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(108, 0, 53.660100216819586)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_max = max([len(i) for i in tqdm(data['correct'])])\n",
    "length_min = min([len(i) for i in tqdm(data['correct'])])\n",
    "avg = [len(i) for i in tqdm(data['correct'])]\n",
    "length_avg = np.array([avg]).mean()\n",
    "\n",
    "length_max , length_min , length_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "255HrENRQvfZ"
   },
   "outputs": [],
   "source": [
    "CORRECT_SENTENCE_LEN = data['correct'].str.split().apply(len) \n",
    "ERRONEOUS_SENTENCE_LEN = data['incorrect'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_b7WjeERiSJ",
    "outputId": "a8cf5026-2b0c-4b80-c33a-6a1e05bddbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "10 6.0\n",
      "20 7.0\n",
      "30 8.0\n",
      "40 10.0\n",
      "50 11.0\n",
      "60 12.0\n",
      "70 14.0\n",
      "80 15.0\n",
      "90 18.0\n",
      "100 25.0\n",
      "90 18.0\n",
      "91 18.0\n",
      "92 18.0\n",
      "93 19.0\n",
      "94 19.0\n",
      "95 20.0\n",
      "96 20.0\n",
      "97 21.0\n",
      "98 21.0\n",
      "99 22.0\n",
      "100 25.0\n",
      "99.1 22.0\n",
      "99.2 23.0\n",
      "99.3 23.0\n",
      "99.4 23.0\n",
      "99.5 23.0\n",
      "99.6 23.0\n",
      "99.7 24.0\n",
      "99.8 24.0\n",
      "99.9 24.0\n",
      "100 25.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,101,10):\n",
    "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))\n",
    "for i in range(90,101):\n",
    "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))\n",
    "for i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n",
    "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E6rKnexSM6o"
   },
   "source": [
    "SINCE 99.9% OF DATA HAS LENGTH LESS THAN 10 , SO SELECTING SENTENCE WITH WORD <10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "ZFwGWKh6SMiy",
    "outputId": "cb1b42a3-33ab-41dc-f42f-1b219c31a1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409715, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f2c2348d-53fd-47b3-83db-e452dbf9dab1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And he took in my favorite subject like soccer .</td>\n",
       "      <td>&lt;start&gt; And he took in my favorite subjects li...</td>\n",
       "      <td>And he took in my favorite subjects like socce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actually , who let me know about Lang   was him .</td>\n",
       "      <td>&lt;start&gt; Actually , he was the one who let me k...</td>\n",
       "      <td>Actually , he was the one who let me know abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>His Kanji 's ability is much better than me .</td>\n",
       "      <td>&lt;start&gt; His Kanji ability is much better than ...</td>\n",
       "      <td>His Kanji ability is much better than mine . &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I heard a sentence last night when I watched TV .</td>\n",
       "      <td>&lt;start&gt; I heard a sentence last night when I w...</td>\n",
       "      <td>I heard a sentence last night when I was watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>When you go downhill , you have to stick out y...</td>\n",
       "      <td>&lt;start&gt; When you go downhill , you have to sti...</td>\n",
       "      <td>When you go downhill , you have to stick out y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2c2348d-53fd-47b3-83db-e452dbf9dab1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f2c2348d-53fd-47b3-83db-e452dbf9dab1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f2c2348d-53fd-47b3-83db-e452dbf9dab1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                           incorrect  \\\n",
       "0   And he took in my favorite subject like soccer .   \n",
       "1  Actually , who let me know about Lang   was him .   \n",
       "2      His Kanji 's ability is much better than me .   \n",
       "4  I heard a sentence last night when I watched TV .   \n",
       "5  When you go downhill , you have to stick out y...   \n",
       "\n",
       "                                         english_inp  \\\n",
       "0  <start> And he took in my favorite subjects li...   \n",
       "1  <start> Actually , he was the one who let me k...   \n",
       "2  <start> His Kanji ability is much better than ...   \n",
       "4  <start> I heard a sentence last night when I w...   \n",
       "5  <start> When you go downhill , you have to sti...   \n",
       "\n",
       "                                         english_out  \n",
       "0  And he took in my favorite subjects like socce...  \n",
       "1  Actually , he was the one who let me know abou...  \n",
       "2  His Kanji ability is much better than mine . <...  \n",
       "4  I heard a sentence last night when I was watch...  \n",
       "5  When you go downhill , you have to stick out y...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CORRECT_SENTENCE_LEN'] = data['correct'].str.split().apply(len)\n",
    "data = data[data['CORRECT_SENTENCE_LEN'] < 20]\n",
    "\n",
    "data['ERRONEOUS_SENTENCE_LEN'] = data['incorrect'].str.split().apply(len)\n",
    "data = data[data['ERRONEOUS_SENTENCE_LEN'] < 20]\n",
    "\n",
    "#ADDING start and end IN THE SENTENCES\n",
    "data['english_inp'] = '<start> ' + data['correct'].astype(str)\n",
    "data['english_out'] = data['correct'].astype(str) + ' <end>'\n",
    "\n",
    "data = data.drop(['correct','CORRECT_SENTENCE_LEN','ERRONEOUS_SENTENCE_LEN'], axis=1)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM8ciYtJWK6f"
   },
   "source": [
    "### Getting train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TfrDw7tzT5Qx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySroEsmXWOFW",
    "outputId": "58791038-ff97-446c-cbbc-8ba8c8cd12ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327772, 3) (81943, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, validation.shape)\n",
    "#ADDING TO <end> TO ONE OF THE SENTENCES SO THAT TOKENIZER LEARNS THE WORD <end>\n",
    "train.iloc[0]['english_inp']= str(train.iloc[0]['english_inp'])+' <end>'\n",
    "train.iloc[0]['english_out']= str(train.iloc[0]['english_out'])+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b05db-RnH_NV",
    "outputId": "f62ecafb-5458-4804-b10d-ddfa2608ec28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327772/327772 [00:00<00:00, 1823171.74it/s]\n",
      "100%|██████████| 327772/327772 [00:00<00:00, 1710270.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 116\n"
     ]
    }
   ],
   "source": [
    "max_encoder_length = max([len(i) for i in tqdm(train['incorrect'])])\n",
    "max_decoder_length = max([len(i) for i in tqdm(train['english_inp'])])\n",
    "print(max_encoder_length , max_decoder_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "Su69ZPzTsxmn",
    "outputId": "afa87bee-e6ac-46cc-8728-43aba6a0b637"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d597ff30-b164-4378-8247-673d34b95e78\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>426921</th>\n",
       "      <td>Could you teach me right ways ?</td>\n",
       "      <td>&lt;start&gt; Could you tell me a right way ?</td>\n",
       "      <td>Could you tell me a right way ?  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115732</th>\n",
       "      <td>there was Music  fountain .</td>\n",
       "      <td>&lt;start&gt; There was a music fountain .</td>\n",
       "      <td>There was a music fountain . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d597ff30-b164-4378-8247-673d34b95e78')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d597ff30-b164-4378-8247-673d34b95e78 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d597ff30-b164-4378-8247-673d34b95e78');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                               incorrect  \\\n",
       "426921  Could you teach me right ways ?    \n",
       "115732       there was Music  fountain .   \n",
       "\n",
       "                                     english_inp  \\\n",
       "426921  <start> Could you tell me a right way ?    \n",
       "115732      <start> There was a music fountain .   \n",
       "\n",
       "                                   english_out  \n",
       "426921  Could you tell me a right way ?  <end>  \n",
       "115732      There was a music fountain . <end>  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "CGqQDR8FWV3D",
    "outputId": "670340d8-25ee-4404-b688-1978cbf7efe4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5c66a6fd-0f13-412d-a99d-1960acb48fbf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>308611</th>\n",
       "      <td>They needs help .</td>\n",
       "      <td>&lt;start&gt; They need help .</td>\n",
       "      <td>They need help . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388907</th>\n",
       "      <td>I have the tennis match tomorrow ,</td>\n",
       "      <td>&lt;start&gt; I have a tennis match tomorrow ,</td>\n",
       "      <td>I have a tennis match tomorrow , &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c66a6fd-0f13-412d-a99d-1960acb48fbf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5c66a6fd-0f13-412d-a99d-1960acb48fbf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5c66a6fd-0f13-412d-a99d-1960acb48fbf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                 incorrect  \\\n",
       "308611                   They needs help .   \n",
       "388907  I have the tennis match tomorrow ,   \n",
       "\n",
       "                                     english_inp  \\\n",
       "308611                  <start> They need help .   \n",
       "388907  <start> I have a tennis match tomorrow ,   \n",
       "\n",
       "                                   english_out  \n",
       "308611                  They need help . <end>  \n",
       "388907  I have a tennis match tomorrow , <end>  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnjtTfVOJLnA"
   },
   "source": [
    "**TOKENIING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GPa0ldDWkav8"
   },
   "outputs": [],
   "source": [
    "tknizer_ERRONEOUS_SENTENCE = Tokenizer()\n",
    "tknizer_ERRONEOUS_SENTENCE.fit_on_texts(train['incorrect'].values)\n",
    "tknizer_CORRECT_SENTENCE = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_CORRECT_SENTENCE.fit_on_texts(train['english_inp'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBb0yEYlCynz",
    "outputId": "d3b2ea1f-d2f1-4e1e-b0a4-9342124d348c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45186\n",
      "59023\n"
     ]
    }
   ],
   "source": [
    "vocab_size_CORRECT_SENTENCE=len(tknizer_CORRECT_SENTENCE.word_index.keys())\n",
    "print(vocab_size_CORRECT_SENTENCE)\n",
    "vocab_size_ERRONEOUS_SENTENCE=len(tknizer_ERRONEOUS_SENTENCE.word_index.keys())\n",
    "print(vocab_size_ERRONEOUS_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm1uoAC-Nwuk",
    "outputId": "4a699818-d4bf-4a49-fbd4-fe738ced1ec6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 23110)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknizer_CORRECT_SENTENCE.word_index['<start>'], tknizer_CORRECT_SENTENCE.word_index['<end>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jnVtXGEJglN"
   },
   "source": [
    "TOKENIZER WITH ENGLISH WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKInpM9fI7TA",
    "outputId": "07707cd4-ca4b-49dc-cb03-f891ac3268f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45187, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/content/drive/MyDrive/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size_CORRECT_SENTENCE+1, 100))\n",
    "for word, i in tknizer_CORRECT_SENTENCE.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "48GmIsiBuV-N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "impressive-advancement"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\", input_shape=(self.vocab_size,))\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        input_embedd                        = self.embedding(input_sentances)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "effective-world"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # we are using embedding_matrix weights and not training the embedding layer\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_decoder\", weights=[embedding_matrix],input_shape=(self.vocab_size,))\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, target_sentances, state_h, state_c):\n",
    "        target_embedd           = self.embedding(target_sentances)\n",
    "        lstm_output, _,_        = self.lstm(target_embedd, initial_state=[state_h, state_c])\n",
    "        return lstm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cathedral-graham"
   },
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "macro-senator"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, max_len):\n",
    "        self.encoder_inps = data['incorrect'].values\n",
    "        self.decoder_inps = data['english_inp'].values\n",
    "        self.decoder_outs = data['english_out'].values\n",
    "        self.tknizer_CORRECT_SENTENCE = tknizer_CORRECT_SENTENCE\n",
    "        self.tknizer_ERRONEOUS_SENTENCE = tknizer_ERRONEOUS_SENTENCE\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_ERRONEOUS_SENTENCE.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "featured-offense"
   },
   "outputs": [],
   "source": [
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        \n",
    "        return [batch[0],batch[1]],batch[2]\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "medium-letter"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "horizontal-links"
   },
   "outputs": [],
   "source": [
    "class vanilla_model(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__() \n",
    "        self.encoder = Encoder(vocab_size=vocab_size_ERRONEOUS_SENTENCE + 1, embedding_dim=100, input_length=encoder_inputs_length, enc_units=256)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size_CORRECT_SENTENCE + 1, embedding_dim=100, input_length=decoder_inputs_length, dec_units=256)\n",
    "        self.dense   = Dense(output_vocab_size, activation='softmax')\n",
    "        \n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neither-tennis",
    "outputId": "90a10b56-d866-4b46-f2dd-e0b6749adfb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 20) (512, 20) (512, 20)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=512)\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "postal-portrait"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "earlier-africa"
   },
   "outputs": [],
   "source": [
    "vanilla = vanilla_model(encoder_inputs_length=16,decoder_inputs_length=16,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
    "vanilla.compile(optimizer= optimizer, loss= loss_function, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "fewer-career"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irgjLAerI2Ac",
    "outputId": "fe1125e8-4323-43ee-b09b-ec974e94df83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 80\n"
     ]
    }
   ],
   "source": [
    "train_steps=train.shape[0]//1024\n",
    "valid_steps=validation.shape[0]//1024\n",
    "print(train_steps,valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjcMmTHC0S9o",
    "outputId": "173e29a7-a3a8-4599-ccb5-3c093acbe075"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "320/320 [==============================] - 145s 431ms/step - loss: 3.2724 - val_loss: 2.9987\n",
      "Epoch 2/60\n",
      "320/320 [==============================] - 145s 454ms/step - loss: 2.8177 - val_loss: 2.6502\n",
      "Epoch 3/60\n",
      "320/320 [==============================] - 147s 460ms/step - loss: 2.5475 - val_loss: 2.4288\n",
      "Epoch 4/60\n",
      "320/320 [==============================] - 148s 461ms/step - loss: 2.3436 - val_loss: 2.2421\n",
      "Epoch 5/60\n",
      "320/320 [==============================] - 140s 436ms/step - loss: 2.1721 - val_loss: 2.0816\n",
      "Epoch 6/60\n",
      "320/320 [==============================] - 149s 465ms/step - loss: 2.0164 - val_loss: 1.9369\n",
      "Epoch 7/60\n",
      "320/320 [==============================] - 144s 449ms/step - loss: 1.8789 - val_loss: 1.7971\n",
      "Epoch 8/60\n",
      "320/320 [==============================] - 147s 458ms/step - loss: 1.7482 - val_loss: 1.6805\n",
      "Epoch 9/60\n",
      "320/320 [==============================] - 143s 446ms/step - loss: 1.6347 - val_loss: 1.5655\n",
      "Epoch 10/60\n",
      "320/320 [==============================] - 149s 467ms/step - loss: 1.5295 - val_loss: 1.4656\n",
      "Epoch 11/60\n",
      "320/320 [==============================] - 144s 450ms/step - loss: 1.4402 - val_loss: 1.3838\n",
      "Epoch 12/60\n",
      "320/320 [==============================] - 144s 451ms/step - loss: 1.3673 - val_loss: 1.3181\n",
      "Epoch 13/60\n",
      "320/320 [==============================] - 147s 461ms/step - loss: 1.2952 - val_loss: 1.2487\n",
      "Epoch 14/60\n",
      "320/320 [==============================] - 149s 466ms/step - loss: 1.2379 - val_loss: 1.1971\n",
      "Epoch 15/60\n",
      "320/320 [==============================] - 147s 459ms/step - loss: 1.1804 - val_loss: 1.1397\n",
      "Epoch 16/60\n",
      "320/320 [==============================] - 146s 456ms/step - loss: 1.1316 - val_loss: 1.0876\n",
      "Epoch 17/60\n",
      "320/320 [==============================] - 143s 445ms/step - loss: 1.0789 - val_loss: 1.0411\n",
      "Epoch 18/60\n",
      "320/320 [==============================] - 150s 469ms/step - loss: 1.0376 - val_loss: 1.0033\n",
      "Epoch 19/60\n",
      "320/320 [==============================] - 150s 469ms/step - loss: 1.0004 - val_loss: 0.9695\n",
      "Epoch 20/60\n",
      "320/320 [==============================] - 144s 451ms/step - loss: 0.9601 - val_loss: 0.9300\n",
      "Epoch 21/60\n",
      "320/320 [==============================] - 150s 470ms/step - loss: 0.9291 - val_loss: 0.9003\n",
      "Epoch 22/60\n",
      "320/320 [==============================] - 144s 451ms/step - loss: 0.8972 - val_loss: 0.8706\n",
      "Epoch 23/60\n",
      "320/320 [==============================] - 146s 457ms/step - loss: 0.8652 - val_loss: 0.8445\n",
      "Epoch 24/60\n",
      "320/320 [==============================] - 140s 437ms/step - loss: 0.8368 - val_loss: 0.8130\n",
      "Epoch 25/60\n",
      "320/320 [==============================] - 143s 447ms/step - loss: 0.8102 - val_loss: 0.7840\n",
      "Epoch 26/60\n",
      "320/320 [==============================] - 144s 449ms/step - loss: 0.7906 - val_loss: 0.7594\n",
      "Epoch 27/60\n",
      "320/320 [==============================] - 151s 471ms/step - loss: 0.7684 - val_loss: 0.7385\n",
      "Epoch 28/60\n",
      "320/320 [==============================] - 150s 468ms/step - loss: 0.7397 - val_loss: 0.7095\n",
      "Epoch 29/60\n",
      "320/320 [==============================] - 144s 451ms/step - loss: 0.7216 - val_loss: 0.6959\n",
      "Epoch 30/60\n",
      "320/320 [==============================] - 146s 456ms/step - loss: 0.7010 - val_loss: 0.6837\n",
      "Epoch 31/60\n",
      "320/320 [==============================] - 148s 461ms/step - loss: 0.6826 - val_loss: 0.6744\n",
      "Epoch 32/60\n",
      "320/320 [==============================] - 145s 454ms/step - loss: 0.6690 - val_loss: 0.6600\n",
      "Epoch 33/60\n",
      "320/320 [==============================] - 139s 434ms/step - loss: 0.6566 - val_loss: 0.6488\n",
      "Epoch 34/60\n",
      "320/320 [==============================] - 143s 447ms/step - loss: 0.6420 - val_loss: 0.6269\n",
      "Epoch 35/60\n",
      "320/320 [==============================] - 144s 451ms/step - loss: 0.6245 - val_loss: 0.6115\n",
      "Epoch 36/60\n",
      "320/320 [==============================] - 147s 459ms/step - loss: 0.6137 - val_loss: 0.6036\n",
      "Epoch 37/60\n",
      "320/320 [==============================] - 146s 455ms/step - loss: 0.5986 - val_loss: 0.5896\n",
      "Epoch 38/60\n",
      "320/320 [==============================] - 148s 463ms/step - loss: 0.5909 - val_loss: 0.5792\n",
      "Epoch 39/60\n",
      "320/320 [==============================] - 143s 448ms/step - loss: 0.5773 - val_loss: 0.5679\n",
      "Epoch 40/60\n",
      "320/320 [==============================] - 146s 455ms/step - loss: 0.5626 - val_loss: 0.5561\n",
      "Epoch 41/60\n",
      "320/320 [==============================] - 139s 436ms/step - loss: 0.5600 - val_loss: 0.5477\n",
      "Epoch 42/60\n",
      "320/320 [==============================] - 149s 466ms/step - loss: 0.5471 - val_loss: 0.5337\n",
      "Epoch 43/60\n",
      "320/320 [==============================] - 150s 469ms/step - loss: 0.5361 - val_loss: 0.5260\n",
      "Epoch 44/60\n",
      "320/320 [==============================] - 144s 449ms/step - loss: 0.5310 - val_loss: 0.5182\n",
      "Epoch 45/60\n",
      "320/320 [==============================] - 146s 455ms/step - loss: 0.5195 - val_loss: 0.5003\n",
      "Epoch 46/60\n",
      "320/320 [==============================] - 140s 438ms/step - loss: 0.5164 - val_loss: 0.4938\n",
      "Epoch 47/60\n",
      "320/320 [==============================] - 143s 445ms/step - loss: 0.5068 - val_loss: 0.4903\n",
      "Epoch 48/60\n",
      "320/320 [==============================] - 140s 438ms/step - loss: 0.5033 - val_loss: 0.4866\n",
      "Epoch 49/60\n",
      "320/320 [==============================] - 143s 448ms/step - loss: 0.4932 - val_loss: 0.4855\n",
      "Epoch 50/60\n",
      "320/320 [==============================] - 146s 457ms/step - loss: 0.4854 - val_loss: 0.4785\n",
      "Epoch 51/60\n",
      "320/320 [==============================] - 148s 463ms/step - loss: 0.4797 - val_loss: 0.4653\n",
      "Epoch 52/60\n",
      "320/320 [==============================] - 145s 453ms/step - loss: 0.4755 - val_loss: 0.4583\n",
      "Epoch 53/60\n",
      "320/320 [==============================] - 147s 459ms/step - loss: 0.4653 - val_loss: 0.4549\n",
      "Epoch 54/60\n",
      "320/320 [==============================] - 150s 468ms/step - loss: 0.4585 - val_loss: 0.4467\n",
      "Epoch 55/60\n",
      "320/320 [==============================] - 146s 457ms/step - loss: 0.4522 - val_loss: 0.4423\n",
      "Epoch 56/60\n",
      "320/320 [==============================] - 146s 455ms/step - loss: 0.4480 - val_loss: 0.4353\n",
      "Epoch 57/60\n",
      "320/320 [==============================] - 148s 462ms/step - loss: 0.4478 - val_loss: 0.4294\n",
      "Epoch 58/60\n",
      "320/320 [==============================] - 139s 435ms/step - loss: 0.4379 - val_loss: 0.4217\n",
      "Epoch 59/60\n",
      "320/320 [==============================] - 150s 470ms/step - loss: 0.4304 - val_loss: 0.4181\n",
      "Epoch 60/60\n",
      "320/320 [==============================] - 145s 452ms/step - loss: 0.4289 - val_loss: 0.4180\n",
      "Model: \"vanilla_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  6280868   \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  4902168   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  11658805  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,841,841\n",
      "Trainable params: 22,841,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla  = vanilla_model(encoder_inputs_length=20,decoder_inputs_length=20,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "vanilla.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
    "train_steps=train.shape[0]//1024\n",
    "valid_steps=validation.shape[0]//1024\n",
    "#TRANING THE MODEL FOR 50 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
    "vanilla.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=60 , validation_data=train_dataloader, validation_steps=valid_steps )#, callbacks=[stp, chkpt, tfboard]\n",
    "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
    "vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W36gv_QP0O1U"
   },
   "outputs": [],
   "source": [
    "os.mkdir('saved_model_newww')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mDA7LIXvBwh"
   },
   "outputs": [],
   "source": [
    "vanilla.save_weights('saved_model_newww/vanilla.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6UQOxDPBDeu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFcQC8r3GD2T"
   },
   "outputs": [],
   "source": [
    "vanilla.load_weights(\"//content/drive/MyDrive/saved_model_newww/vanilla.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3O-hpedF3iN",
    "outputId": "36629dd1-c066-4006-bd3e-7b7b75c9947a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vanilla_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  6280868   \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  4902168   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  11658805  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,841,841\n",
      "Trainable params: 22,841,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vanilla.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SFjlz-AG1t4"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "DZ_QeXvxG1Wu",
    "outputId": "ee024986-5871-4f5e-d16f-c422bcb9a989"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<end> '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(enc_inp,dec_inp):\n",
    "        \n",
    "    translation=\"\"\n",
    "\n",
    "    e_input=[]\n",
    "    for i in enc_inp.split():\n",
    "        if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "            e_input.append(0)\n",
    "        else:\n",
    "            e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "    #decoder input\n",
    "    d_input=[]\n",
    "    for i in dec_inp.split():\n",
    "        if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "            d_input.append(0)\n",
    "        else:\n",
    "            d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "    for word in prediction[0]:\n",
    "        word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "    translation += word + \" \"\n",
    "    \n",
    "    return translation\n",
    "a = train.incorrect[4]\n",
    "b =  train.english_inp[4]\n",
    "pred = inference(a,b)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3otXDvr4-_F",
    "outputId": "99ca3f46-0003-4f45-8489-302bd280f12e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Actually , who let me know about Lang   was him .',\n",
       " '<start> Actually , he was the one who let me know about Lang   . .')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.incorrect[1] , train.english_inp[1] , "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfuQQ1Kk41Ia"
   },
   "outputs": [],
   "source": [
    "enc_inp = train.incorrect[1]\n",
    "dec_inp =  train.english_inp[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "HP239Hia4uAE",
    "outputId": "3e8533f2-05d6-4324-9d9a-a52a4d065c61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'who who who has happy reason who do me '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation=\"\"\n",
    "\n",
    "e_input=[]\n",
    "for i in enc_inp.split():\n",
    "    if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "        e_input.append(0)\n",
    "    else:\n",
    "        e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "#e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "#there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "#decoder input\n",
    "d_input=[]\n",
    "for i in dec_inp.split():\n",
    "    if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "        d_input.append(0)\n",
    "    else:\n",
    "        d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "#d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "for word in prediction[0]:\n",
    "    word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "    if word == \"<end>\":\n",
    "        break\n",
    "    translation += word + \" \"\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8o6HWhTFkw1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nBF1J4nK4f_4",
    "outputId": "6178a9a7-26ac-4a6b-c100-8c37ec26f34f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-188ad5d4-e9d7-4d34-a339-455fd0ef2469\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>434830</th>\n",
       "      <td>Since I had no class in my university , I went...</td>\n",
       "      <td>&lt;start&gt; Even though I had no class in my unive...</td>\n",
       "      <td>Even though I had no class in my university , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65366</th>\n",
       "      <td>We should chose the fact .</td>\n",
       "      <td>&lt;start&gt; We should choose the fact .</td>\n",
       "      <td>We should choose the fact . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112798</th>\n",
       "      <td>Massive earthquakes hit Japan while ago ! X</td>\n",
       "      <td>&lt;start&gt; Massive earthquakes hit Japan a while ...</td>\n",
       "      <td>Massive earthquakes hit Japan a while ago ! X ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263823</th>\n",
       "      <td>Tired but feeling good .</td>\n",
       "      <td>&lt;start&gt; I 'm tired but feeling good .</td>\n",
       "      <td>I 'm tired but feeling good . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321778</th>\n",
       "      <td>this is four days , and this is going to bigin...</td>\n",
       "      <td>&lt;start&gt; It is four days long , and it is going...</td>\n",
       "      <td>It is four days long , and it is going to begi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329331</th>\n",
       "      <td>and he is characterized in this film as a grum...</td>\n",
       "      <td>&lt;start&gt; He is characterized in this film as a ...</td>\n",
       "      <td>He is characterized in this film as a grumpy m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66596</th>\n",
       "      <td>It 's getting cooler after the typhoon has gone .</td>\n",
       "      <td>&lt;start&gt; It 's getting cooler now that the typh...</td>\n",
       "      <td>It 's getting cooler now that the typhoon has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266425</th>\n",
       "      <td>It is ours .</td>\n",
       "      <td>&lt;start&gt; It is ours</td>\n",
       "      <td>It is ours &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259369</th>\n",
       "      <td>Every Saturday , my school holds football event .</td>\n",
       "      <td>&lt;start&gt; Every Saturday , my school holds a foo...</td>\n",
       "      <td>Every Saturday , my school holds a football ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216529</th>\n",
       "      <td>I found the interesting calendar there .</td>\n",
       "      <td>&lt;start&gt; I found an interesting calendar there .</td>\n",
       "      <td>I found an interesting calendar there . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-188ad5d4-e9d7-4d34-a339-455fd0ef2469')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-188ad5d4-e9d7-4d34-a339-455fd0ef2469 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-188ad5d4-e9d7-4d34-a339-455fd0ef2469');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                incorrect  \\\n",
       "434830  Since I had no class in my university , I went...   \n",
       "65366                          We should chose the fact .   \n",
       "112798       Massive earthquakes hit Japan while ago ! X    \n",
       "263823                           Tired but feeling good .   \n",
       "321778  this is four days , and this is going to bigin...   \n",
       "...                                                   ...   \n",
       "329331  and he is characterized in this film as a grum...   \n",
       "66596   It 's getting cooler after the typhoon has gone .   \n",
       "266425                                      It is ours .    \n",
       "259369  Every Saturday , my school holds football event .   \n",
       "216529           I found the interesting calendar there .   \n",
       "\n",
       "                                              english_inp  \\\n",
       "434830  <start> Even though I had no class in my unive...   \n",
       "65366                 <start> We should choose the fact .   \n",
       "112798  <start> Massive earthquakes hit Japan a while ...   \n",
       "263823              <start> I 'm tired but feeling good .   \n",
       "321778  <start> It is four days long , and it is going...   \n",
       "...                                                   ...   \n",
       "329331  <start> He is characterized in this film as a ...   \n",
       "66596   <start> It 's getting cooler now that the typh...   \n",
       "266425                                 <start> It is ours   \n",
       "259369  <start> Every Saturday , my school holds a foo...   \n",
       "216529    <start> I found an interesting calendar there .   \n",
       "\n",
       "                                              english_out  \n",
       "434830  Even though I had no class in my university , ...  \n",
       "65366                   We should choose the fact . <end>  \n",
       "112798  Massive earthquakes hit Japan a while ago ! X ...  \n",
       "263823                I 'm tired but feeling good . <end>  \n",
       "321778  It is four days long , and it is going to begi...  \n",
       "...                                                   ...  \n",
       "329331  He is characterized in this film as a grumpy m...  \n",
       "66596   It 's getting cooler now that the typhoon has ...  \n",
       "266425                                   It is ours <end>  \n",
       "259369  Every Saturday , my school holds a football ev...  \n",
       "216529      I found an interesting calendar there . <end>  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train.sample(1000)\n",
    "sample_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "T-NAasQ3Q3Jh",
    "outputId": "9482ae91-3ab9-484f-a5c3-35b08e7c15e9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'you you go downhill you you have to stick out your chest or go will go down '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(enc_inp,dec_inp):\n",
    "            \n",
    "    translation=\"\"\n",
    "\n",
    "    e_input=[]\n",
    "    for i in enc_inp.split():\n",
    "        if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
    "            e_input.append(0)\n",
    "        else:\n",
    "            e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
    "\n",
    "\n",
    "    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
    "\n",
    "    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
    "\n",
    "    #decoder input\n",
    "    d_input=[]\n",
    "    for i in dec_inp.split():\n",
    "        if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
    "            d_input.append(0)\n",
    "        else:\n",
    "            d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
    "\n",
    "    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
    "\n",
    "    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
    "\n",
    "    for word in prediction[0]:\n",
    "        word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "        translation += word + \" \"\n",
    "    return translation\n",
    "a = train.incorrect[5]\n",
    "b =  train.english_inp[5]\n",
    "pred = inference(a,b)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7noJ9eFPS8DY",
    "outputId": "cf261562-d36b-47ff-eb45-38184e4cfa48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The making souvenir is a hard and interesting work .',\n",
       " '<start> Making souvenirs is a hard but interesting work .',\n",
       " 'making making is making hard and interesting and ')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.incorrect[10]\n",
    "b =  train.english_inp[10]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlrh_8nNTDEB",
    "outputId": "b6faaf04-411d-4c49-faf5-e63f808e2634"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"That 's two balls : one with a cat inside , other with dog .\",\n",
       " '<start> There are two balls : one with a cat inside , the other with dog .',\n",
       " 'two two two balls with with with a cat inside with with other with him ')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train.incorrect[12]\n",
    "b =  train.english_inp[12]\n",
    "pred = inference(a,b)\n",
    "a  , b , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txAQpSyVThEs",
    "outputId": "df7d5369-d975-447c-f8ff-e35dee21fe7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:19<00:00, 12.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['is is not more than but it it it it is a castle named castle castle ',\n",
       " 'was was was against ',\n",
       " 'of of you know know know how to appreciate the taste of gourmet food ',\n",
       " 'you you ever say to you ',\n",
       " \"a a a 's team team won the game game game and the champions champion yesterday \",\n",
       " 'helps helps helps us with communicating with foreigners ',\n",
       " 'certainly certainly it certainly certainly as with the one one one ',\n",
       " 'dentist dentist got rid of the bad part of the tooth ',\n",
       " 'all all all boil boil soy beans ',\n",
       " 'because those hostels have a community room ',\n",
       " 'take take my dog and walk out for a walk on the beach ',\n",
       " 'everyone everyone i i i hope hope wish time i the start i the the ',\n",
       " 'and of getting up early ',\n",
       " 'my my my junior boy shooting was there and and got a strong and and a big applause ',\n",
       " 'if if you are not satisfied with my current job now now now ',\n",
       " 'that that there a a a ban a smoking may as and could not be be just out ',\n",
       " \"god god god 's title title title title for the \",\n",
       " 'the the is to translate simple simple sentence sentence ',\n",
       " \"honestly honestly n't often koreans frequently ' one other ' \",\n",
       " 'to read ',\n",
       " 'they they always gather water but but the nobody ordered it ',\n",
       " \"a a a a i i i been away because i my husband 's business \",\n",
       " 'took took online online online lesson for the first time in two weeks ',\n",
       " 'is is that incredible souvenir for me ',\n",
       " 'ordinary ordinary ordinary ordinary people who regular the health insurance cost the the the the total cost of of ',\n",
       " 'wanted wanted to encourage her to to her sweet smile ',\n",
       " 'if if i i up my my feelings feelings be than ',\n",
       " 'i i many clothes from ',\n",
       " 'last last last last last the last yesterday cancel the flowers for for ',\n",
       " 'i i i up it it it it ',\n",
       " \"did did did her thorough examination but then could n't find anything anything \",\n",
       " 'a a event is like a festival ',\n",
       " 'and talk very smoothly ',\n",
       " 'could could that the to have a very good time at that party ',\n",
       " \"is is his daughter 's name \",\n",
       " 'one one one was that the dead dead dead ',\n",
       " 'that that listened listened listened to music ',\n",
       " 'should should should are have the ability to solve problems problems ',\n",
       " 'hope hope that my is is a long time ',\n",
       " 'that that found that my was to me while i i so ',\n",
       " 'met met one of my senior yesterday ',\n",
       " 'felt felt sad in the in their they lost their friends and i in arms ',\n",
       " 'it it it is hard hard me hard for him to catch him ',\n",
       " 'tomorrow our tomorrow ',\n",
       " 'i i her friend sister today ',\n",
       " 'can can can use people people people people be with people around all the world ',\n",
       " 'after graduating from the school in in in in one one a year to study ',\n",
       " 'becareful becareful a care care ',\n",
       " 'going going going to go to a a a business trip again ',\n",
       " 'see see you last summer in the garden ',\n",
       " 'will will it it in cofusion and and the composition will have never the the theme ',\n",
       " 'my my my computer and i i going to finish writing ',\n",
       " 'i i muscle muscle muscle muscle ',\n",
       " 'it it on the the computer but but there was nothing ',\n",
       " \"' ' ' ' ' ' ' ' ' ' ' ' around around \",\n",
       " 'children children are so hard ',\n",
       " 'was was heartwarming to see them ',\n",
       " \"can can understand the they 're doing about the or \",\n",
       " 'we we will in in this country country country for more ',\n",
       " 'as as normally usual ',\n",
       " 'thinking thinking anything ideas ideas abruptly comes to my mind ',\n",
       " \"do do n't like these events because it it it are so \",\n",
       " 'just just finished my work until now ',\n",
       " 'his his his his master and of him him to to become a pro identity ',\n",
       " 'they they are going to buy some food ',\n",
       " 'please please take us for for sightseeing ',\n",
       " 'have have have have have a lot of trouble crossing the street ',\n",
       " '',\n",
       " \"cigarettes cigarettes cigarettes cigarettes is expensive is is n't it \",\n",
       " 'the the is blackouts ',\n",
       " 'was was the first time that that driven her car ',\n",
       " 'i i relieved relief them hear them ',\n",
       " 'was was gambling gambling ',\n",
       " 'first first to to to to say yes yes ok ',\n",
       " 'i i i to to not be study but but also enjoy learning it ',\n",
       " 'you you know ',\n",
       " 'were were talking on many things ',\n",
       " 'the the fireworks finished a a slight ashes started shooting ',\n",
       " 'take take take a taxi to my friend ',\n",
       " 'in in in in senior high school paid paid paid all my attation to school school work ',\n",
       " 'the the the type is the the sea shell with the sea shore and i view ',\n",
       " \"something something i i i definitely be for my family 's good known just just \",\n",
       " 'did did you you it it ',\n",
       " 'from from each wedding were brilliant and conjured up a lovely of nostalgia for missing fleeting moment ',\n",
       " 'i i i i so bored ',\n",
       " 'most most typhoons move from the path to the move move move they move to north north ',\n",
       " 'i i worried about the weather in the ',\n",
       " 'so so so so also close watching so why likely and start learning english english ',\n",
       " 'i i i i not good at ',\n",
       " 'i i is is a very beautiful song ',\n",
       " 'i i my entries entries entries ',\n",
       " 'eventually eventually watched until the end of the game ',\n",
       " 'another another another another that that like about to more children to study more more ',\n",
       " 'i i i that communication using the is the most important thing thing ',\n",
       " \"could could n't even stand with my eyes open \",\n",
       " 'you you a come a where the the buy one this year ',\n",
       " 'my my accepted my vacation if vacation vacation early early early ',\n",
       " 'they they the best friends ',\n",
       " 'is is not so exciting as i i ',\n",
       " '',\n",
       " 'i i to speak smoothly smoothly ',\n",
       " 'today today today today today today there there there is no thing there there is no tomorrow ',\n",
       " 'will will will will be rainy or snowy ',\n",
       " 'really really enjoyed the fact but but but but but ',\n",
       " 'really really really is is be able to take a long for such a long time ',\n",
       " 'nor nor nor course nor nor or or neither neither ',\n",
       " 'these these days reserchers reserchers stayed my lab recently recently recently recently ',\n",
       " 'it it a like a a drama ',\n",
       " 'write write this letter the the ',\n",
       " 'we we had money problem problem today but but we solved the problem ',\n",
       " 'i i to the supermarket even though it day was so so good ',\n",
       " 'also useful for people people will ',\n",
       " 'so so so a lecture to to the an teacher license ',\n",
       " 'i i i i to improve my poor poor poor have sense progress ',\n",
       " \"haha haha this 's i i \",\n",
       " \"but he could n't find it at all and did did the doctor \",\n",
       " \"it it it it it it n't 's hard to argue that his his his his girlfriend \",\n",
       " 'stayed stayed at the ryokan the the ocean view the from our room ',\n",
       " 'i i up at am am and and ate a hot noodle for breakfast ',\n",
       " 'my my my my writing writing skills ',\n",
       " 'there there there there is a risk of buy on the ',\n",
       " 'have have i a bad relationship with my boss ',\n",
       " 'i i i learning but but but not not not at good at ',\n",
       " 'or or or it this write this this this my opinion ',\n",
       " 'are are you talking about ',\n",
       " 'do do you have about about ',\n",
       " 'had had had no idea where what that ',\n",
       " 'it it feel so cool ',\n",
       " 'high high high in college college the college of exams ',\n",
       " 'in in countries study study study hard and prepare to get a job ',\n",
       " 'i i i i and and and throw away away ',\n",
       " 'encouraging encouraging me on passing ',\n",
       " 'now now i i how difficult it is ',\n",
       " \"we we it 's n't affect our health in the bad term \",\n",
       " 'let and cure ',\n",
       " 'please please to to you have a to are to doctor or australian student ',\n",
       " 'classes classes classes classes classes from now now and pm pm ',\n",
       " 'what what what my could teacher was me the the ',\n",
       " 'are are many messages in the lyrics and the melodies ',\n",
       " 'that that my my parents did did did not know the cause of my body health for am am am ',\n",
       " 'i boss expects an ',\n",
       " 'also also also importantly importantly seem difficult is is difficult to use particles ',\n",
       " 'thinking thinking wondering about raising raising children ',\n",
       " 'his his his his him and and i to cheer him ',\n",
       " 'ever ever there were a lot of merchandise ',\n",
       " 'it it very long and there was in the pond ',\n",
       " 'dear dear dear dear ',\n",
       " 'people people eat this kind of things everyday ',\n",
       " 'hope hope it like and and enjoy it it movie with this in this weekend ',\n",
       " 'the the of parts is me is speaking ',\n",
       " 'huang seems seems like bull bull ',\n",
       " 'because people who know the couple from more countries gather and share their memories ',\n",
       " 'work work hard hard to finish at at ',\n",
       " 'i i i i i to read news news news ',\n",
       " 'i i that people people people can can can live as as ',\n",
       " 'it it today ',\n",
       " 'as was was saiate sleep dance ',\n",
       " 'i i done my st free time on ',\n",
       " 'that that dream dream dream everyday everyday with her ',\n",
       " 'it it it it it it ',\n",
       " 'would would rather be as as such more than a the sense of obligation ',\n",
       " 'went went went on trips trip ',\n",
       " 'i i i feel lonely ',\n",
       " 'this this this this day in japan japan an japan because of the earthquake but but ',\n",
       " 'the the a lake is by a beaches and ocean ',\n",
       " 'why why why the feeling sensation of itchiness exists ',\n",
       " 'i i that i i i to overcome my weak point no matter ',\n",
       " 'to to be improved because my son is only year old months old old ',\n",
       " 'jog jog seven foreteen foreteen twenty twenty twenty seconds seconds ',\n",
       " 'my my my level level can can can not say something that who saying saying or or are ',\n",
       " \"it it it 's time to go to the piano lesson \",\n",
       " 'the the the was opponent was ',\n",
       " 'on on hand professors professors who work outside will help which is is really useful in factual projects ',\n",
       " \"i i for his birthday 's present but but we could n't \",\n",
       " \"haha i i i i n't gone to sleep \",\n",
       " 'i i looking forward to reading it ',\n",
       " 'fact fact not not not so far far from ',\n",
       " 'people people can understand their mind thoughts well anytime time ',\n",
       " 'regretted regretted regretted regretted regretted juice juice ',\n",
       " 'it it it an itch between my toes ',\n",
       " 'have have to fill three three pages ',\n",
       " 'like like like to stay there for a girls towns towns ',\n",
       " 'the the the peaceful right everything looks asleep except except except except for me ',\n",
       " 'has has is to people any who have you have many more and me ',\n",
       " 'there there there there a group and ate it ',\n",
       " 'who who who tube tube this read comments here this greeting greeting ',\n",
       " 'have have have to help her study ',\n",
       " 'is is a person who has many many things to learn ',\n",
       " 'i i that reason for for i i i a pedometer ',\n",
       " 'the the the train caught fire and the passengers escaped from the tunnel on foot ',\n",
       " 'will will have no classes tomorrow ',\n",
       " 'is is a lot of time that i very exciting ',\n",
       " 'and soaked in a hot spring ',\n",
       " 'say say make make criminals hang themselves ',\n",
       " 'i i buy it next time ',\n",
       " 'another another is another grape hyacinth it it it like grape they they usually usually flowers ',\n",
       " 'we we have to wait for eat it ',\n",
       " 'rather rather cooking cake and chocolate cupcakes ',\n",
       " 'am am hometown hometown hometown hometown in an an with with ',\n",
       " 'is is the start of a new week and study on a ',\n",
       " 'came came came back to that that that about that it was very ridiculous ',\n",
       " \"have have lots of things i i to have have have have n't been all at all \",\n",
       " 'the the following two three were were were anime movies on ',\n",
       " 'for for these these people ',\n",
       " 'so i i study english english learn english get high english english in in ',\n",
       " 'it it is really annoying ',\n",
       " 'do do do will use use the emoticons can can can clearly my emotions clearly ',\n",
       " 'test test test and chicken chicken ',\n",
       " 'make make sleep sleep lightly and also stresses your visceral ex ',\n",
       " 'near near near near near my house to to off the soil dried will save it ',\n",
       " 'was was really frustrated about a slow slow response ',\n",
       " 'i i smoked made pork pork eggs eggs cheese cheese and octopus ',\n",
       " 'but but but but but ',\n",
       " 'also also also to also bank and i office ',\n",
       " 'about about my like of my picture ',\n",
       " 'if if you you home at work work work work work at at sleep at night ',\n",
       " 'next next will will go it for for ',\n",
       " \"police police people that the 's true after after he investigated the victim 's death \",\n",
       " 'and after dinner was was was to the local center which is located in california ',\n",
       " 'i i ',\n",
       " 'the the of this trip is to eat ramen ramen and i do where grave of clothing ',\n",
       " 'are are the traditional way way of greeting ',\n",
       " 'is is a story about a young person who searches for a dream paradise ',\n",
       " 'it it it was rainy but but but but we went out from ',\n",
       " \"in in n't a special one we we were just of the common couples \",\n",
       " \"it it 's good good way to learn \",\n",
       " 'now now i i confused about using conjunctions hmm hmm hmm hmm ',\n",
       " 'i i regretful ',\n",
       " 'the the is language ',\n",
       " 'the the the cuisine cuisine does have have specific specific countermeasure ',\n",
       " 'because we we you a label for many through the the the label for the the the ',\n",
       " 'the the the that are concerned about the the audience rate ',\n",
       " \"ca ca ca n't ask what teacher what does mean mean \",\n",
       " 'i i i to have many friends ',\n",
       " 'the the the can be the the to how such you ',\n",
       " \"we we checked the vet 's no but but we could n't see the hell \",\n",
       " 'having having having trouble with my vocabulary vocabulary ',\n",
       " 'many many many many many many many to to ',\n",
       " 'ordinary ordinary find this type of pepper ',\n",
       " 'the the before in in in in ',\n",
       " 'met met i i my friends so so it was a fine day ',\n",
       " 'such as trekking cycling cycling and and playing around the the beach ',\n",
       " 'something something from different are different ',\n",
       " 'read read a book of a ',\n",
       " 'then he he looked to watch a professional soccer game in this evening ',\n",
       " 'want want to learn it though though though i i someone likes ',\n",
       " 'help help me learn english english english ',\n",
       " 'it it are to start ',\n",
       " 'like like to surf the websites both both honesty honesty accurate accurate accurate ',\n",
       " 'i i i i studied slang slang on slang and idioms ',\n",
       " 'i i to do a language exchange on skype skype skype ',\n",
       " 'have have have have a plan to go to r r r r r ',\n",
       " 'did did much but but but i i too tired to write this ',\n",
       " 'a a years ago i i i addicted to mystery novels ',\n",
       " 'have have have to study to to to to to to write this starting today ',\n",
       " 'is is becoming a super aging society ',\n",
       " 'sleeping sleeping sometimes sleep more more more take a long for more than minutes ',\n",
       " 'are are going to see on ',\n",
       " 'on on on on on the sea of of noodles ',\n",
       " \"now now my my my husband 's hair \",\n",
       " \"if if if you are carefully you you should n't to \",\n",
       " 'the the said have have to people people people who can able to speak and read well ',\n",
       " 'for for your for your friends and learn ',\n",
       " 'you you play this game you you must experience the the hero temples kyoto legacy and beautiful place ',\n",
       " 'after after after a few days just just started our of our daily ',\n",
       " 'and the is spreading rapidly ',\n",
       " 'it it it it it been a day since i i and wrote my entries ',\n",
       " 'this this this post a my th account ',\n",
       " 'we we lunch lunch we we went to until until we until exams until am was ',\n",
       " 'which which which was ',\n",
       " 'you you have a problem in the the the the answer ',\n",
       " 'what the is the answer ',\n",
       " 'impression impression was a nice nice nice nice school ',\n",
       " 'and and and and ',\n",
       " 'i i i i a cat called a a a brother and and and a dog called ',\n",
       " 'are are a lot of japanese japanese japanese japanese japanese this this ',\n",
       " 'is is is is is coming ',\n",
       " 'have have to get ready to take a train train ',\n",
       " 'makes makes make me to understand familiar with more passage quickly quickly ',\n",
       " 'is is the what what is wrong these these are not important to me ',\n",
       " 'think think keeping is work is really important to learn students ',\n",
       " 'talked talked about my career plans with my boss ',\n",
       " 'the the was given a notice of the from the last of the previous name ',\n",
       " 'think think it it will will be fun ',\n",
       " 'was was was was a freshman in a company participated joining joining this project ',\n",
       " 'so so so sorry about the tragedy ',\n",
       " 'haveconfirmed haveconfirmed the message belowand amreally interested in this opportunity ',\n",
       " 'like like like weekends weekend so much ',\n",
       " 'is is the time for youth ',\n",
       " 'it it it it much warmer there that that that ',\n",
       " 'the the of the stickam post plan plan plan how plan how how he make a girlfriend ',\n",
       " 'it it very hard but fun too ',\n",
       " \"i i to of 's my kids ' s \",\n",
       " 'physics physics physics physics experiment ',\n",
       " 'i i lucky to find found here ',\n",
       " 'an an an an an interesting video ',\n",
       " 'are are getting old ',\n",
       " \"on on 's a plate \",\n",
       " 'he he and his team drove to the at the speed of each us ',\n",
       " 'work work work hard especially especially ',\n",
       " 'could could not move anything even even though inch in the heavy packed train ',\n",
       " 'i i so happy now ',\n",
       " 'my my my room ',\n",
       " 'the the of this line said that practice sessions were paid each day ',\n",
       " 'they they they they made lot of dumplings buns buns and biscuits ',\n",
       " 'this this evening will will will have a big meal for my family ',\n",
       " 'why why why students try the best for the entrance examinations of university ',\n",
       " 'snow snow snow but but dislike cold ',\n",
       " 'it it a little ',\n",
       " 'like like like to look down to the audience floor ',\n",
       " 'the the the minister ',\n",
       " 'i i i on my train train these these these that my atmosphere around me is different ',\n",
       " 'i i to eat sandwiches for the sake of a lot of nutrients ',\n",
       " 'there there are things which do do not do any any way as can can not understand ',\n",
       " 'for for that that is interesting interesting that that that the it the from the a the has has ',\n",
       " 'me me the right answer ',\n",
       " 'you you heard of this name name ',\n",
       " 'think think can can go this as as a study study ',\n",
       " 'the the to the sports shop after work ',\n",
       " 'i i wondering ',\n",
       " 'this this my first diary entry entry ',\n",
       " 'too too too too hot to a mental work ',\n",
       " 'two two women in the early twenties ',\n",
       " 'for for send friends who sent me messages ',\n",
       " 'the the the parade today ',\n",
       " 'i i a sister old sister ',\n",
       " 'use use use it was was was been used ppc ',\n",
       " 'mr mr mr taken she took to me and then looked so happy ',\n",
       " \"i i to go to bed in p m m but but but but it it n't \",\n",
       " 'we we we all about our family about this matter ',\n",
       " 'an an showed showed showed showed me some of of the the ',\n",
       " 'was was the at the same time p p ',\n",
       " 'trying trying to write something in english english english ',\n",
       " 'has has not gone out at all when my school has ',\n",
       " 'earthquakes earthquakes earthquakes has has occurred experienced yesterday yesterday pm pm yesterday ',\n",
       " 'do do you follow the opinion of others ',\n",
       " 'the the end of we we we are going to get our bonus ',\n",
       " 'it it often in case in your country too ',\n",
       " 'think think think is is the real complicated language in the world except only are on on the ',\n",
       " 'i i very happy and helpful for me to read this sentences ',\n",
       " 'for for i if me ',\n",
       " 'and we talked about the great neural neural neural ',\n",
       " '',\n",
       " 'i i a lot of sheep and and took photographs ',\n",
       " 'everybody everybody showed up so so asking asking checks wait until until the answer of time it time ',\n",
       " 'looking looking looking for teaching teaching job ',\n",
       " 'feel feel feel feel very dizzy ',\n",
       " \"i i to do volunteeer guide than it it it it 's difficult to write the \",\n",
       " 'young young smokers think it is cool smoking smoking can help them to attractive ',\n",
       " 'is is reality ',\n",
       " 'it it it was to the extra amount to to that that ',\n",
       " 'first first at at first ',\n",
       " 'have have have have never had a before ',\n",
       " 'got got a day off friday friday ',\n",
       " 'it it to take a long time ',\n",
       " 'can can can know one information on on on the ',\n",
       " 'is is one of my favorite and and and that other another reason like i i this is ',\n",
       " 'i i years old male male ',\n",
       " 'the the the bus stop and saw saw saw people waiting waiting for the bus ',\n",
       " 'are are many arguments related the idea that the is more important than friends ',\n",
       " 'of of all it it are hot and their their height and their hansom looks ',\n",
       " 'i i spain spain will win the championship ',\n",
       " 'using using using for businesses ',\n",
       " 'learning learning is is is easy for me ',\n",
       " 'i i use a a a a ',\n",
       " 'hello hello hello hello hello ',\n",
       " 'think think i i more exercise ',\n",
       " 'we we should put moisturizer on our skin ',\n",
       " 'my my favorite of favorite season is been summer ',\n",
       " 'should should should the game should not to to ',\n",
       " 'i i i to improve my skills skills ',\n",
       " 'no no one announced this this year too ',\n",
       " 'she she she says to him love love love you ',\n",
       " 'even even if i i a higher score in i or or ',\n",
       " 'hard hard hard independently everyday ',\n",
       " \"have have 's n't pain \",\n",
       " 'trying trying trying try to write a a little thing ',\n",
       " \"buzz sounds sounds would more going going n't get up up to get out \",\n",
       " \"did did n't know what happened \",\n",
       " 'is is one of theories or techniques ',\n",
       " 'the the the to the answer ',\n",
       " 'have have have have a few friends who can how to paint here ',\n",
       " 'this this cause of the oral ulcers ',\n",
       " 'took took a lot of pictears pictures with my friends ',\n",
       " 'find find a very nice to it it ',\n",
       " 'i i i that that beautiful the morning was was pleasant pleasant confortable river there is ',\n",
       " 'i i this website helps me my new new new ',\n",
       " 'when when when the the out of my mouth felt felt felt cold cold of cold wind ',\n",
       " 'btw btw btw senior member has also a smoker ',\n",
       " 'it it was natural substance ',\n",
       " 'i i to to to understand ',\n",
       " 'my my break break break break spent a of time at home ',\n",
       " 'my friend gave me advice to how to roast steak if ',\n",
       " 'the the the phone ',\n",
       " 'we we we we invited two of our friends to dinner ',\n",
       " 'heard heard explanations from a few companies ',\n",
       " 'the the club are practicing as i i i looking forward to the concert ',\n",
       " 'world world changes rapidly by the day day ',\n",
       " \"it it does n't care maybe might think about that \",\n",
       " 'i i i i about this site on on on facebook ',\n",
       " 'i i i i not drive quickly very well ',\n",
       " 'ielts ielts is over ',\n",
       " \"at at the online lesson lesson could could could n't say what really really wanted to say \",\n",
       " 'my my my university life started started started playing and playing playing playing ',\n",
       " 'i i working for a foreign company ',\n",
       " \"is is n't easy to bring my \",\n",
       " 'i i i i be going go to stay stay work ',\n",
       " 'my my my up mind in a a new life as a dream dream ',\n",
       " 'but every connection in phone my body is me feel feel up ',\n",
       " 'is is the second time ',\n",
       " 'the the the the listening section was when when the quality section was terrible ',\n",
       " 'it it been raining since i morning ',\n",
       " 'this this one that makes people like me feel sad ',\n",
       " 'the the process of pursuing ',\n",
       " 'first first first first say our school earlier earlier than ten ten ',\n",
       " 'like like a higher sound like it looks like like kyuweeeeen ',\n",
       " 'he is wangshi maybe he is famous but but he he never heard of him ',\n",
       " 'god god i that it was snowing ',\n",
       " 'try try to write a english diary because i i some goals ',\n",
       " 'it it not good at the up stuff ',\n",
       " \"i i n't even even even i i in in college college \",\n",
       " 'i i very busy right now who who have have a decide on a course graduation graduation ',\n",
       " 'i i i to get together with my classmates and professors ',\n",
       " 'took took the bus to to our our own in our apartment ',\n",
       " 'is is nice tool for communicating with many people and and a ',\n",
       " 'the the started not so good but but the reminder was good ',\n",
       " 'am am very busy about whether to to complete the full distance ',\n",
       " 'i i started to studying studying study ',\n",
       " 'let let make sentences sentence that use use that if not made made made made any mistake ',\n",
       " 'but he is an outgoing person to and and and and he looks very tired ',\n",
       " 'i i a bit angry and he is is busy enjoying enjoying the concert ',\n",
       " 'was was very happy day ',\n",
       " 'took took took a picture of a a is a famous this year ',\n",
       " 'we we we insert some coins ',\n",
       " 'this this that people express express their opinions in their clothes ',\n",
       " 'these these their which which that having and so on ',\n",
       " 'rain rain light the sky and so the air clearer ',\n",
       " 'for steamed dumplings it was good but but it was not good for example fried dumplings ',\n",
       " 'i i to continue to perfectly because it it a test ',\n",
       " 'give give give to give my voice a time in in order to go our school tomorrow ',\n",
       " \"pink pink flowers flowers people people and and everyone 's this this honor \",\n",
       " 'realized realized that now is is not living ',\n",
       " 'heard heard heard heard from from my friend ',\n",
       " 'his his house and and drank and played the video game ',\n",
       " 'had had a good sleeping even even though i i up up at midnight ',\n",
       " 'have have have heard ',\n",
       " 'remember remember he he the fish like was at a fish ',\n",
       " \"this is n't the way \",\n",
       " 'i i my lovely teacher ',\n",
       " 'is is a so so like like game such as cf cf cf animal and on on ',\n",
       " 'all all all people are love love very friendly to strangers like me ',\n",
       " 'is is not so far so so it it it a long trip ',\n",
       " 'how how did you think any true interests for a job ',\n",
       " \"she she did n't buy goods \",\n",
       " \"the the graduation graduation graduation thesis has n't completed completed \",\n",
       " \"today today is saturday saturday saturday saturday n't go on school 's 's \",\n",
       " 'was was lying to reading a entry entry under the tree ',\n",
       " 'golf golf is rational ',\n",
       " 'to to to suppress to my idea to him him ',\n",
       " 'you you have a good way to say ',\n",
       " 'for for beautiful beautiful beautiful beautiful beautiful pictures and and beautiful scenery ',\n",
       " 'i i to my best and and and and i i i ',\n",
       " 'anpanman anpanman the rival rival in in his garden ',\n",
       " 'i i to improve my friends friends meet people who want to speak ',\n",
       " 'i i middle of my my my my i to my my meet my best friend ',\n",
       " 'do do you feel about changing the job ',\n",
       " 'i i to continue my this this ',\n",
       " 'or or do exercises exercises better scientific techniques ',\n",
       " 'love love its story concept retro retro taste and and music ',\n",
       " 'there there afternoon and and and had a in mosque ',\n",
       " 'the the the words that that you are also to to ',\n",
       " 'there there are primaries or caucuses ',\n",
       " 'they they were really delicious especially especially an orange pizza and fried fried chicken ',\n",
       " 'not not just just took a walk ',\n",
       " \"the words or phrases in the parenthesis refer to the us does does n't understand \",\n",
       " 'is is not a college student working working hard everyday and i i it it it ',\n",
       " 'i i glad that have a a new diary here here ',\n",
       " 'love love are are is of of the flower ',\n",
       " 'some some some dates and we came married one year later ',\n",
       " 'does does does does does it have this feature the the sound ',\n",
       " 'the the plays the songs in the beach in the the ',\n",
       " \"do do n't know whether he uses this word consciously or not \",\n",
       " 'opportunity opportunity for for hour ',\n",
       " 'mystery mystery and feelings have improved when i i when it ',\n",
       " 'for for me my errors ',\n",
       " 'can can you you feel cold inside the bone ',\n",
       " 'it it some long and thick cucumbers ',\n",
       " 'got got got a money thousand wons but but this year is becomes more than before year ',\n",
       " 'i i i i studying real english english english ',\n",
       " 'the activities in are held in the afternoon ',\n",
       " 'it it it is my own decision ',\n",
       " 'when when try try to write in my my my brain stops working ',\n",
       " 'heard heard great great news ',\n",
       " 'must must have felt more scary than me ',\n",
       " 'you you have use up on frame into a ballpoint pen ',\n",
       " 'hosted hosted an american american in the th ',\n",
       " 'did did it sewing today i i i a lot lot about the world financial meltdown ',\n",
       " \"i i worried about he did n't understand what i i \",\n",
       " 'i i inconceivably sick ',\n",
       " 'i i it is not problem because ',\n",
       " 'collect collect money on many things and send a to a countries ',\n",
       " \"they they taught me which if you you n't give me my my dream come come true \",\n",
       " 'is is introducing introducing introduce myself the the easiest theme of of all ',\n",
       " 'lucky lucky was was was was was was ',\n",
       " 'it it it is be yummy ',\n",
       " 'buying buying houses ',\n",
       " 'it it it it it difficult to speak fluently fluently ',\n",
       " 'have have have passed work as a teacher teacher ',\n",
       " 'i i i to improve my language skills that that that ',\n",
       " 'usually usually stay at our place on weekend ',\n",
       " 'i i i i to take myself to see able to cry by in an appropriate occasion ',\n",
       " 'the the evening until the it it heavy has been heavy ',\n",
       " 'i i happy ',\n",
       " 'yesterday yesterday yesterday yesterday muscles ache ache ',\n",
       " \"to to to to to park at o ' clock \",\n",
       " 'it it a really tough event ',\n",
       " 'about about me ',\n",
       " 'invited invited me people including my parents and hers ',\n",
       " 'the the of the middle presentation was was social social social networking ',\n",
       " 'work work for a company where the the the are from other countries ',\n",
       " '',\n",
       " 'year year year old children were more than i i ',\n",
       " '',\n",
       " 'essay essay is the tackle for finish for the end end during the end of this month ',\n",
       " 'pour in canned tomatoes and put a a bouillon cube ',\n",
       " 'i i on on on the road and then there wrong could not there there ',\n",
       " 'love love apples and bananas ',\n",
       " 'the the he was was really attractive to to other words unbelievable unbelievable ',\n",
       " 'i i part a part time job today ',\n",
       " 'worrying worrying worrying about about many people people are the disaster area ',\n",
       " 'i i to write this diary again ',\n",
       " 'today i i i up at am am ',\n",
       " 'the the the is not for say unbelievable unbelievable ',\n",
       " 'go i i to every every year ',\n",
       " 'on on that that talk what what we should do after this ',\n",
       " 'g g is a little in in in ',\n",
       " 'i i nice ',\n",
       " 'since since never drank a drink since because it it not so to get it in ',\n",
       " 'ok ok ok p p p p p ',\n",
       " 'have have this be these project on time ',\n",
       " 'finding finding been looking for some new roommates so now ',\n",
       " 'has has a cat a a dog and two gerbils ',\n",
       " 'had had to look for she daugter had had dinner and a bath ',\n",
       " 'i i admire your insight about national character ',\n",
       " \"it 's dead \",\n",
       " 'and and and last last last last festival in in in my town too too ',\n",
       " 'it it so much but a bit scared he he lost him way ',\n",
       " 'wait wait for a minute for me ',\n",
       " 'first first the with seurat seurat back to when she she still my my eyes ',\n",
       " 'coming neural neural neural ',\n",
       " 'cons cons of icing tea weather appearing poverty and and a tough of work to do ',\n",
       " 'we we we at the station station received received received a text message from him ',\n",
       " \"is is n't the same time that that something like this \",\n",
       " 'i i the number more more than they price as ago ',\n",
       " 'man man ',\n",
       " 'like like music music such such as a music music music music music music ',\n",
       " 'is is really a frustrating work for me ',\n",
       " 'they they that astronauts are now taking the drinking of reprocessed urine in space ',\n",
       " '',\n",
       " 'is is to to ',\n",
       " 'the the question is that he they really spies ',\n",
       " 'when when stress stress stressed that that that makes me stressed ',\n",
       " 'i i very happy to hear that ',\n",
       " 'maybe maybe be because it was a a sample ',\n",
       " 'people people people people were were just only and just ',\n",
       " \"do do n't give up \",\n",
       " \"i i i in so so i i n't been reading for for about \",\n",
       " 'the the the main dish dish was not so bad ',\n",
       " 'i i i been busy and interviews and the for a job ',\n",
       " 'listened listened listened to the numbers in his speech ',\n",
       " 'it it snowing snow snow away away been a a since since ',\n",
       " 'there there hole there there they they mend it quickly ',\n",
       " 'the the season of that that that is called tsuyu disgusts disgusts everybody ',\n",
       " 'i i i keep a diary entry week so my to improve my english english english ',\n",
       " 'and and and ',\n",
       " 'i i i i a student student ',\n",
       " 'this this about about a drug scandal we often hear these days ',\n",
       " 'my my my habits have changed since the past the past ',\n",
       " 'i i i to make friends with everyone ',\n",
       " 'but but most important reason is that ',\n",
       " 'to to to play soccer so i wants to play with with me ',\n",
       " 'was was absent from my job ',\n",
       " 'hmmm but but sometimes she she you to music ',\n",
       " \"decided decided to go there but but no do do n't really want to go \",\n",
       " 'will will have a debate class two two weeks ',\n",
       " 'foreign foreign languages is difficult ',\n",
       " 'there there was no information about the aquarium there ',\n",
       " 'i i very very ',\n",
       " 'it it so very and big place ',\n",
       " 'golf golf golf ',\n",
       " 'am am i waiting for her answer ',\n",
       " 'at at at underground underground there there is some free trial with with a few pages ',\n",
       " 'time time time time ',\n",
       " 'i i up late with my yesterday ',\n",
       " 'my my body itches so much without any reason ',\n",
       " 'the the of this movie is antagonism between nature and human culture ',\n",
       " 'can can can help other people with russian russian russian ',\n",
       " 'who who tell me the reason why this situation ',\n",
       " 'there there were ',\n",
       " 'a a good night ',\n",
       " 'sorry sorry sorry to to to you worry so much about my early leaving ',\n",
       " 'snow snow board the was this season ',\n",
       " 'want want to expand my vocabulary and write more complex sentences ',\n",
       " 'it it is better to to to my my chest altered there ',\n",
       " '',\n",
       " 'advertisement advertisement advertisement advertisement should be be prohibited ',\n",
       " 'are are two of on ',\n",
       " 'can can only only only only read ',\n",
       " 'learning learning learning hydrotechnical engineering ',\n",
       " 'i i an awful score ',\n",
       " 'basic basic it it is hard to my stiff body to complete the the the tasks ',\n",
       " 'think think the negative feelings have me have such dream ',\n",
       " 'have have have have have studied study hard until the end of the the ',\n",
       " 'and and and points points are points ',\n",
       " 'i i to go tomorrow work tomorrow ',\n",
       " 'rescue rescue rescue operation is finished at the disaster of disaster ',\n",
       " 'was was moved to i i that movie and thought thought that that that wanted to do something ',\n",
       " \"i i i n't find a job for for for nothing to do everyday \",\n",
       " 'i i ',\n",
       " 'i i i i i to dance dance a dance club members ',\n",
       " 'scary scary ',\n",
       " 'the the of is comes comes comes comes going i go back to the times ',\n",
       " 'i i have go out for before my flight ',\n",
       " \"do do n't know what my mistakes \",\n",
       " 'my my in my class was be in school school school ',\n",
       " 'contrast contrast the the other tends tend to keep silent ',\n",
       " 'i i i i sorry for her ',\n",
       " 'the the is is is is is very handsome ',\n",
       " 'he he to talk more about it it ',\n",
       " 'i i to write a my diary again ',\n",
       " 'then please tell me if she can when when she ispreparing to come ',\n",
       " \"because because natural because n't work \",\n",
       " 'can can learn learn from from them ',\n",
       " 'was was was embarrassing to discuss with my classmates first first ',\n",
       " 'i i i cooking is to love and and i to my family ',\n",
       " 'bassists bassists have have a much ',\n",
       " 'of of of the the the the the the the the ',\n",
       " 'i i likely fall fall asleep ',\n",
       " 'hope hope i i like like paper ',\n",
       " 'it it it it it there go there ny with ny sleeves ',\n",
       " 'rarely rarely do this non much much of lot of time to use used to the future ',\n",
       " 'if if if if to to to to to not not to with anymore ',\n",
       " 'three three three relationships are necessary knowledge knowledge honesty honesty and and has sense of humor ',\n",
       " 'besides besides besides mother just commanded me to wash all of dirty bags and glasses ',\n",
       " 'ran ran from to to to to to and and back ',\n",
       " 'the the the situation has improved improved considerably but but it for time for for ',\n",
       " 'the the following of the piece had had had mixed feelings of sadness and anger ',\n",
       " 'are are too many missing missing still ',\n",
       " 'what what what would would to tell is that the is the most precious time in life ',\n",
       " 'why why why i i to new cellphone ',\n",
       " 'thanks thanks feel good and refreshed from now ',\n",
       " 'usually usually usually usually usually usually usually grow than ',\n",
       " 'to to to have to do it just heat ',\n",
       " 'of of many i i i tomatoes into omlet ',\n",
       " 'is is the picture of grilled eels ',\n",
       " 'so i i a little bit tired ',\n",
       " 'time time time did not allow me to come ',\n",
       " \"the the was very well so so did did n't have time for lunch \",\n",
       " 'during during weather forecast typhoons typhoons are coming coming coming ',\n",
       " 'm to be m m it it it the biggest dinner for me these these days few days ',\n",
       " 'could could a moon moon ',\n",
       " 'know know the opening song ',\n",
       " 'often often i to the for for her job and went went with with her ',\n",
       " 'the the me me usual sentences and and me me a little shortened with its function ',\n",
       " 'i i been studying for for many years ',\n",
       " 'it it been a long time since my my visit my favorite german book store ',\n",
       " 'i i i i to express my opinions clearly clearly ',\n",
       " 'really really hate hate like my brother ',\n",
       " 'you you want talk to me in russian or english welcome welcome to me english english english ',\n",
       " 'ate ate famous noodles noodles ',\n",
       " 'she she felt down because of the result but but the the of was natural consequences ',\n",
       " 'feel feel that the time passes on the ',\n",
       " 'the the process we we talked about our futures ',\n",
       " \"all all all all are all all all 's 's to us \",\n",
       " \"do do you is is is n't long a long dairy \",\n",
       " 'usually usually illuminate trees and houses during the holiday season ',\n",
       " 'the the thrower ',\n",
       " 'the the was cheap but but ',\n",
       " 'i i we will be working at in the near future ',\n",
       " \"do do n't play the piano \",\n",
       " 'will will to be to fast to i i i ',\n",
       " 'i i a year old university university student and active law and politics ',\n",
       " 'the the director the statement of interactive function related my phones exists ',\n",
       " \"is is not a historical event event so so do do n't known the manner of this holiday \",\n",
       " 'i i to work early morning to the in the afternoon ',\n",
       " 'his his his his is is he is only a but but the has lots child love love ',\n",
       " 'long neural neural ',\n",
       " 'tags tags exist only new new products ',\n",
       " 'she joked to to to to marry married to you ',\n",
       " 'are are less less tourists than or or or ',\n",
       " 'you you know the the is tuba is ',\n",
       " 'work work at a net cafe ',\n",
       " 'various various various types of baths outside and outside of the outside ',\n",
       " 'i i i i ',\n",
       " 'my my my friend ',\n",
       " \"do do n't like the and are people people people and noisy \",\n",
       " 'is is is club activities ',\n",
       " 'want want to be more fluent ',\n",
       " \"but but but do honest did did n't have any idea about what we we \",\n",
       " 'there there there people there there and and modern buildings ',\n",
       " 'joined joined it few hours ago ',\n",
       " 'is is very bad at speaking but english english english i i in ',\n",
       " 'i i gon na eat it it tonight ',\n",
       " 'is is a very artistic picture ',\n",
       " 'the the is about my next trip to talk talk talk talk ',\n",
       " 'think think about the employment overseas overseas ',\n",
       " 'see neural neural ',\n",
       " 'roses roses grew and their flowerpots are too small for them ',\n",
       " 'i i i i feeling today today ',\n",
       " 'you you know some good websites site to me me me me know ',\n",
       " 'simply simply eat the rice boiled boiled rice and noodles also a balls and it ',\n",
       " 'it seems that efforts are unfruitful because their bosses are corrupt ',\n",
       " 'it it it very very good product but but ',\n",
       " \"do do n't know the correct way \",\n",
       " 'just just just joined lang ',\n",
       " 'taking taking present some examples methods ',\n",
       " 'next next next year year much much plenty of time to study it it ',\n",
       " 'of of of course course that the the a song of praise to live ',\n",
       " 'what what what the prince ',\n",
       " 'my my my english skill skill have improved a little ',\n",
       " 'watch watch watching movies from and brothers brothers brothers brothers brothers brothers ',\n",
       " 'will will will a good for i weather thing is not ',\n",
       " 'it it eggs egg bowl ',\n",
       " 'i i i to one one one one of the most areas ',\n",
       " 'the the went interested interested interest common to to to interesting interested in classical and french some books ',\n",
       " 'could could not tell him anything ',\n",
       " 'people people come together together ',\n",
       " \"i i n't care about that problem but but ca ca n't help each about our future \",\n",
       " 'it it been a tough month ',\n",
       " 'live live in a village near near and and there there far visited tokyo in mountains ',\n",
       " 'my my brother told me that he is planning to ',\n",
       " 'i i to write about my diary for months months ',\n",
       " 'my my told me that his friend on on my camera ',\n",
       " 'i i so so bad bad ',\n",
       " 'the the were talking about the country ',\n",
       " 'time time passed passed was was was six years ago so so i i to learn again ',\n",
       " 'i i i i a the bus in my first time in my life ',\n",
       " 'a a a a car after a for a long interval ',\n",
       " 'like like to to better than the the ',\n",
       " 'came came came home ',\n",
       " 'is is the test and and first diary ',\n",
       " 'my my is a a a a a a a a a is a small island ',\n",
       " 'it it was a litttle too long for me ',\n",
       " 'children children go to school from today ',\n",
       " 'it it a great time ',\n",
       " 'i i to apply for the ',\n",
       " 'you you send sending an an article ',\n",
       " \"we we we we though we did n't decide on the general for for \",\n",
       " 'have have begun me me me my life life a a ',\n",
       " 'we we we are not so are ',\n",
       " 'we we we we had to clean the butter because shaking it with water ',\n",
       " 'what what what advice ',\n",
       " 'my my old old to invite her to a restaurant or club ',\n",
       " 'i i very moved to famous single men skating ',\n",
       " 'try try trying to speak it it it ',\n",
       " 'because because is probably ',\n",
       " \"i i n't play it time it \",\n",
       " \"as as as as old way way may think it 's so that i i anime will \",\n",
       " 'fact fact is the the the the violation of the labor ',\n",
       " \"if you do n't have confidence in my skills skills now \",\n",
       " \"he dinner was was was supposed to do homework homework homework but could did n't stay up \",\n",
       " 'i i ',\n",
       " 'i i i pass a examination examination examination ',\n",
       " 'does does she did not reached reached the legal drinking age yet ',\n",
       " 'is is a holiday so so i i answer the comments of the entries entries of me ',\n",
       " 'i i registered on this site today so learn how to write well ',\n",
       " 'this this this is harder than i i ',\n",
       " 'i i tried tried to stop anything something ',\n",
       " 'i i i to do a useful work work ',\n",
       " \"wo wo wo n't get a high score limit limit limit be discouraged \",\n",
       " \"i i felt felt felt let it it 's down \",\n",
       " 'instead instead instead instead it to to instead instead ',\n",
       " 'put put put the of of the market of the year the the ',\n",
       " 'did did he he he he he he started saying ',\n",
       " \"could could n't regret it because we we a meeting \",\n",
       " \"because one of my friends had to to do and could n't come to us \",\n",
       " 'now now thanks thanks to the information information information can find information quickly quickly ',\n",
       " 'reached reached to yesterday yesterday ',\n",
       " 'is is so expensive especially especially for us ',\n",
       " 'you you check my sentences very very very be very happy ',\n",
       " 'really really like the situation and the and the waves waves when i i sailing ',\n",
       " \"did did n't done to continue it because because i i been busy and and and moving \",\n",
       " 'have have tried to be truthful with him ',\n",
       " 'thank thank be about you when you are worried ',\n",
       " \"did of of n't have time to write \",\n",
       " 'i i to improve my dream dream my dream to studying abroad and around around the world ',\n",
       " 'decided decided decided to overhaul it by myself ',\n",
       " 'ice ice cream ice snowing weather taste really good ',\n",
       " 'could could able to come back home ',\n",
       " 'and and java java two java used java languages ',\n",
       " 'my my in the abilities are very skilled ',\n",
       " 'the the friends who are the attendees of the kindergarten party party last night ',\n",
       " 'i i i i i to a activity of raising money ',\n",
       " 'give give give an outline of the story ',\n",
       " 'was was the the same place for a long time like like i i that happened here here ',\n",
       " 'the the past always always always brought my lunch with me ',\n",
       " 'its its its that its thick and made by chemical textiles ',\n",
       " 'parents parents also refused his stupid and and he he a and cares about their own citizen citizen ',\n",
       " 'film film was especially especially especially the characters of the main actress ',\n",
       " 'is is is coming around the corner ',\n",
       " 'i i use it for a trial ',\n",
       " 'the the on this trip is on and will for my work ',\n",
       " \"she she said n't told me to to to buy bought only mic mic without mic mic \",\n",
       " 'is is my english time ',\n",
       " 'i i i not make those kind of list for me ',\n",
       " 'it it very scary ',\n",
       " 'my my son got a stye in his eye ',\n",
       " 'i i i i out of my bed and closed my window ',\n",
       " 'toilet toilet is taken of our ',\n",
       " 'the the boy boy has gone gone ',\n",
       " 'the the movie of of mysterious of an accidents and my my my my my favorite scene ',\n",
       " 'the the the the like this ',\n",
       " 'had had had had free time and finally arrived there there ',\n",
       " 'can can only two two days a one month ',\n",
       " 'i i seen the posters where the young boy is everywhere in times ',\n",
       " 'positive positive positive positive positive enough positive high quality that that can think of other people ',\n",
       " 'the the result of of of people could the page ',\n",
       " 'today today today today ',\n",
       " 'this this time of of course my my parents will join the wedding party ',\n",
       " 'i i a happy thing ',\n",
       " 'feeling feeling feeling a bit sexy so we we can later ',\n",
       " 'you you you as a lab third you as you you ',\n",
       " 'taught taught me me me and me i me to pass the entrance exams ',\n",
       " 'the the the champion crowned ',\n",
       " 'had had my experience several days ago ',\n",
       " \"may may may give you a beautiful face but but it can n't give you a beautiful soul \",\n",
       " \"picture picture is today today 's local newspaper \",\n",
       " 'was was and and was circulation of neonate today ',\n",
       " 'was was was was back from ',\n",
       " 'i i i i a a winter coat recently ',\n",
       " 'thinking thinking writing to complain about it awful service we store assistants provided them ',\n",
       " 'of of proper english confused me ',\n",
       " 'the government announced that todayhas a vacation again ',\n",
       " 'i i i i to the lacoste lacoste i bought a huge i bought and i ',\n",
       " 'can can go back the the foundation and visit some coworkers as well ',\n",
       " 'went went to the hospital with my mom this morning and it it some medicine there ',\n",
       " \"is is is golden ' you you are not not is common common sense rooted to our face \",\n",
       " 'had had had three weeks ago but but it was a big ',\n",
       " \"in in you n't like this situation \",\n",
       " 'like like this application very much ',\n",
       " 'have have an appointment in the the the morning ',\n",
       " 'want want want want to add garlic ',\n",
       " \"do do do n't have chances chance to learn how to write in \",\n",
       " 'it it practice ',\n",
       " \"drunk drunk coffee 's coffee coffee ' m was supremo ' \",\n",
       " \"my my visited visited visited visited my grandparents ' house every summer \",\n",
       " 'my daughter said that she wants to go to theamusement park again ',\n",
       " 'i neural neural ',\n",
       " 'i i i i not surehow how to use this website ',\n",
       " 'the the me a little of for my private life ',\n",
       " \"' ' why ' ' ' we can disuss this ' \",\n",
       " 'i i i i this year to be great ',\n",
       " 'in in students students in our school is that we is effective to learn through this way ',\n",
       " 'i i up ',\n",
       " 'it it already ends end of september september september september september middle exam is in ',\n",
       " 'the the helps me ',\n",
       " 'i i is learning learning learning been learning grammar rules and vocabulary by heart ',\n",
       " 'to to have to pay yen yen ',\n",
       " 'the the weather is not cold not hot ',\n",
       " 'i i to to to advise them ',\n",
       " 'was was was ',\n",
       " 'we we a a lot of fun things haha ',\n",
       " 'was was in the same ',\n",
       " 'can can not remember words which are too much ',\n",
       " 'i i be all ',\n",
       " 'the the latter states the the more have has been been me ',\n",
       " 'students students live in room that has beds tables tables chairs chairs bookcases bookcases ',\n",
       " 'name name name name ',\n",
       " 'i i was was was been too busy to get into lang ',\n",
       " 'is is more common ',\n",
       " 'they they they be it they they they they have always together together each other ',\n",
       " 'i i like to become a criminal figure ',\n",
       " \"do do n't have a printer a a pen tablet and and large a a more powerful \",\n",
       " 'write write write writing about that ',\n",
       " 'i i years year old old ',\n",
       " 'rap rap rap become popular from then ',\n",
       " 'see see the on the web is free ',\n",
       " 'put put put put put some some weight ',\n",
       " \"it it it 's also responsible to charge corporation 's taxes \",\n",
       " 'i i out eating some worms in it ',\n",
       " 'think think think i i be vanish and he he and lay lay lose everything ',\n",
       " 'after after a dentist interrogated the the guy guy who to stealing the money ',\n",
       " \"the the the the world is simpler than when we can n't follow our hearts \",\n",
       " 'though though i i very sleepy ',\n",
       " \"omg omg omg 's completely out though though though that me \",\n",
       " 'giving giving giving them them them them discard put it the the ',\n",
       " 'does does to able to use use without without words ',\n",
       " 'never never gave up and fight against protected the nation ',\n",
       " 'so i i gon na ',\n",
       " 'he he to to euros ',\n",
       " 'my my my classmates have been a beautiful policewoman ',\n",
       " 'had had a lot of fun and good was good exercise ',\n",
       " 'yesterday yesterday yesterday yesterday the football stadium in london london london ',\n",
       " 'it it it it and it it it ',\n",
       " 'from from was difficult difficult difficult problems problems ',\n",
       " 'in in restaurant have regular customers who come often ',\n",
       " 'the the a lot on ',\n",
       " 'though though it it even like them mahjong mahjong is attractive ',\n",
       " 'was was big ',\n",
       " 'how do this company choose candidates for be new students who the candidates choose to the same amount ',\n",
       " 'the the way you think about this world ',\n",
       " \"i i a called called ' ' ' ' ' \",\n",
       " 'i i a happy no no reason ',\n",
       " 'many many many many rice cookers for their boyfriends even even for relatives ',\n",
       " 'there there is customs custom in your country ',\n",
       " 'a a a steamboat this time we we had had a time this year was was ',\n",
       " 'finally finally finally finally ',\n",
       " 'they they wanted to be a personal movement or or anything else ',\n",
       " 'are are sweets ',\n",
       " 'suppose suppose the strong point is linked on the weak point ',\n",
       " 'is is it a observation however however from from my biological perspective can can can be true ',\n",
       " 'developing developing new products of air conditioners make use of technology technology of of a these days ',\n",
       " 'have have a plan for a trip to a country country ',\n",
       " 'i i to go to a school school school high school ',\n",
       " 'e mail is mf mf msn ',\n",
       " 'simulating life with a girlfriend ',\n",
       " 'am am i i connected on the internet internet internet internet internet the internet ',\n",
       " \"the the edo of of of consumers ' objective behavior changed \",\n",
       " 'i i to learn by a speaker speaker ',\n",
       " 'it it very a as a a a ',\n",
       " 'subjects subjects skipped in general conversations like native speakers speakers were about something ',\n",
       " 'i i up my mind to study hard hard would it it fluent ',\n",
       " 'can can understand what they are is only to best ',\n",
       " 'tell tell tell them two second points tomorrow tomorrow and more details ',\n",
       " 'he he his own farms and grows vegetables there ',\n",
       " \"ciao is my cat 's name and it means that \",\n",
       " 'the the thing is to think about the way to solve this situation ',\n",
       " 'they they they glasses ',\n",
       " 'it it very difficult to me to wake up in the morning ',\n",
       " 'the the movie stars stars was i i i ',\n",
       " 'this this this is very fun because i i watching watch the drama ',\n",
       " 'correct correct ',\n",
       " 'the the and tomatoes ',\n",
       " 'and very expensive ',\n",
       " 'will will go and and and buy a a for a friend of ',\n",
       " 'going going going to get a opportunity opportunity license ',\n",
       " 'nobody nobody wants her ',\n",
       " \"i i finished to my friend 's diary \",\n",
       " 'i i starting my my keep the household of the household expenditures ',\n",
       " 'one one a ume plant on my campus today ',\n",
       " 'it it a day day ',\n",
       " 'guys guys guys guys ',\n",
       " 'often often buy something in internet internet ',\n",
       " 'was was was walking around my university suddenly suddenly suddenly so bad ill and she down ',\n",
       " 'days days i i busy for school for preparation for sports ',\n",
       " 'my my diary or some journals and and native native speakers correct sentences ',\n",
       " 'is is the the the the the the the the the the the the the ',\n",
       " 'the the the the least i i wish wish is be hired just just ',\n",
       " 'trying to improve my english ',\n",
       " 'as as like to write as many as as as as ',\n",
       " 'anyway have have have a plan to take the next next next year ',\n",
       " 'you you imagine the weather is keeping in everyday everyday ',\n",
       " 'did did did to know on it ',\n",
       " 'is is snack snack snack food and there are various flavors ',\n",
       " 'i i so so so ',\n",
       " 'i i comments for me comments ',\n",
       " 'i i him at first sight ',\n",
       " 'the the of the medical checkup would there ',\n",
       " 'will will face and different problems ',\n",
       " 'my my said check check check check you mom to put on on ',\n",
       " 'just just just watched the sad story of a series drama series so i i so bad now ',\n",
       " 'the the is the important to be next next next military military military purposes ',\n",
       " 'all all all all of are elderly ',\n",
       " 'it it became be a little bit easier to be hot hot weather ',\n",
       " 'please please me please thx thx thx for you help and and ',\n",
       " 'can can find out of in hiragana hiragana or e book this this ',\n",
       " 'have have started using use skype site site ',\n",
       " 'the the will be released on on on ',\n",
       " 'i i i i never been a a countries so so i i looking forward to visiting there ',\n",
       " 'people people usually the the ask the ',\n",
       " 'the the which which is large large menu menu lunches ',\n",
       " 'my my this year is to improve my language language to to ',\n",
       " 'think think think think that to review the are more important than to write another language language ',\n",
       " 'people people people come to only only only over other other there over other countries ',\n",
       " 'visit visit visit visit ',\n",
       " 'it it very very hot every every day ',\n",
       " 'should should to hide the since cops had followed him ',\n",
       " \"do do do do n't have the capability to do that \",\n",
       " 'even even the hotel should have responsible for the first fee for offered ',\n",
       " 'temporary temporary temporary job for for two two months ',\n",
       " 'was was so dense and tourists and local people ',\n",
       " 'have have have been the is you must to calm calm and you are facing the relationship ',\n",
       " 'made made tomato sauce today ',\n",
       " 'now now it it that but i has never been me bored ',\n",
       " 'ok ok for me correct ',\n",
       " 'speak speak speak speak it it ',\n",
       " 'i i doing my best to recall things about the pharmacy ',\n",
       " \"have have n't realized it since years ago ago \",\n",
       " 'it it it day off today ',\n",
       " 'i i it very much ',\n",
       " 'i i i to i my skills skills ',\n",
       " 'the whole school has been very happy ',\n",
       " 'if if i i can can can can explain explain it well ',\n",
       " 'turning turning turning my poster presentation my presentation was was was been planning on trip ',\n",
       " 'ideas ideas ideas other any important my personal views but but but everyone else ',\n",
       " 'in other words make make make money ',\n",
       " 'i i earnest and persevering ',\n",
       " 'my my is the the very difficult ',\n",
       " 'the the look like like alike ',\n",
       " 'only only know the of actors actors ']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train.sample(1000)\n",
    "result = []\n",
    "i=0\n",
    "for i, j , k in tqdm(sample_train.values):\n",
    "    # print(i , j , k)\n",
    "    pred = inference(i , j)\n",
    "    result.append(pred)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 998
    },
    "id": "uMFeNbysHjWI",
    "outputId": "9a702d96-d7b7-4169-973d-9d8259482ebb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a8394108-9885-484a-a566-dc7ab2ef04a6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "      <th>PREDICTED_SENTENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94511</th>\n",
       "      <td>It is not more famous than Kyoto , but it has ...</td>\n",
       "      <td>&lt;start&gt; It is no more famous than Kyoto , but ...</td>\n",
       "      <td>It is no more famous than Kyoto , but it has a...</td>\n",
       "      <td>is is not more than but it it it it is a castl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370365</th>\n",
       "      <td>Seattle Mariners was against Losangels Angels .</td>\n",
       "      <td>&lt;start&gt; Seattle Mariners were  went againstthe...</td>\n",
       "      <td>Seattle Mariners were  went againsttheLos Ange...</td>\n",
       "      <td>was was was against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365572</th>\n",
       "      <td>Because of you , I knew how to appreciate the ...</td>\n",
       "      <td>&lt;start&gt; Because of you , I knew how to appreci...</td>\n",
       "      <td>Because of you , I knew how to appreciate the ...</td>\n",
       "      <td>of of you know know know how to appreciate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140865</th>\n",
       "      <td>you ever to say with you self ?</td>\n",
       "      <td>&lt;start&gt; Do you ever say to yourself ,</td>\n",
       "      <td>Do you ever say to yourself , &lt;end&gt;</td>\n",
       "      <td>you you ever say to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76135</th>\n",
       "      <td>Japanese women soccer team won the World Cup g...</td>\n",
       "      <td>&lt;start&gt; The Japanese women 's soccer team won ...</td>\n",
       "      <td>The Japanese women 's soccer team won the Worl...</td>\n",
       "      <td>a a a 's team team won the game game game and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346202</th>\n",
       "      <td>The Internet helps us with communicatig with f...</td>\n",
       "      <td>&lt;start&gt; The Internet helps us with communicati...</td>\n",
       "      <td>The Internet helps us with communicating with ...</td>\n",
       "      <td>helps helps helps us with communicating with f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371230</th>\n",
       "      <td>I thought it is certainly compared with the Sa...</td>\n",
       "      <td>&lt;start&gt; I thought it is certainly comparable w...</td>\n",
       "      <td>I thought it is certainly comparable with the ...</td>\n",
       "      <td>certainly certainly it certainly certainly as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318578</th>\n",
       "      <td>The dentist got rid out of the bad part on the...</td>\n",
       "      <td>&lt;start&gt; The dentist got rid of the bad part of...</td>\n",
       "      <td>The dentist got rid of the bad part of the too...</td>\n",
       "      <td>dentist dentist got rid of the bad part of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86311</th>\n",
       "      <td>First of all , boil soy beens .</td>\n",
       "      <td>&lt;start&gt; First of all , boil soy beans .</td>\n",
       "      <td>First of all , boil soy beans . &lt;end&gt;</td>\n",
       "      <td>all all all boil boil soy beans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161455</th>\n",
       "      <td>because , those hostels has a community room ,</td>\n",
       "      <td>&lt;start&gt; because those hostels have a community...</td>\n",
       "      <td>because those hostels have a community room , ...</td>\n",
       "      <td>because those hostels have a community room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291771</th>\n",
       "      <td>We take my dog and go out for a walk on the be...</td>\n",
       "      <td>&lt;start&gt; We took my dog and went out for a walk...</td>\n",
       "      <td>We took my dog and went out for a walk on the ...</td>\n",
       "      <td>take take my dog and walk out for a walk on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403716</th>\n",
       "      <td>Hi everyone hope you have passed the great tim...</td>\n",
       "      <td>&lt;start&gt; Hi everyone , Hope you had a great tim...</td>\n",
       "      <td>Hi everyone , Hope you had a great time with t...</td>\n",
       "      <td>everyone everyone i i i hope hope wish time i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299214</th>\n",
       "      <td>and weak up early .</td>\n",
       "      <td>&lt;start&gt; instead of waking up early .</td>\n",
       "      <td>instead of waking up early . &lt;end&gt;</td>\n",
       "      <td>and of getting up early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>Fortunately , my junior 's magic went well , a...</td>\n",
       "      <td>&lt;start&gt; Fortunately , my junior 's magic went ...</td>\n",
       "      <td>Fortunately , my junior 's magic went well , w...</td>\n",
       "      <td>my my my junior boy shooting was there and and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248236</th>\n",
       "      <td>So if you are not satisfied with your current ...</td>\n",
       "      <td>&lt;start&gt; So if you are not satisfied with your ...</td>\n",
       "      <td>So if you are not satisfied with your current ...</td>\n",
       "      <td>if if you are not satisfied with my current jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24705</th>\n",
       "      <td>Thinking of that , smoking prohibition seems u...</td>\n",
       "      <td>&lt;start&gt; Keeping that in mind , a ban on smokin...</td>\n",
       "      <td>Keeping that in mind , a ban on smoking seems ...</td>\n",
       "      <td>that that there a a a ban a smoking may as and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84179</th>\n",
       "      <td>Thanks god it 's Saturday . Some websites for ...</td>\n",
       "      <td>&lt;start&gt; Thank God it 's Saturday . Some websit...</td>\n",
       "      <td>Thank God it 's Saturday . Some websites for r...</td>\n",
       "      <td>god god god 's title title title title for the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>The practice is to translate the simple Japane...</td>\n",
       "      <td>&lt;start&gt; The practice is to translate a simple ...</td>\n",
       "      <td>The practice is to translate a simple Japanese...</td>\n",
       "      <td>the the is to translate simple simple sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19115</th>\n",
       "      <td>I rarely heard that people use ' one another '</td>\n",
       "      <td>&lt;start&gt; I do n't hear people use  ' one anothe...</td>\n",
       "      <td>I do n't hear people use  ' one another ' very...</td>\n",
       "      <td>honestly honestly n't often koreans frequently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>to read in English .</td>\n",
       "      <td>&lt;start&gt; to read it in English .</td>\n",
       "      <td>to read it in English . &lt;end&gt;</td>\n",
       "      <td>to read</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8394108-9885-484a-a566-dc7ab2ef04a6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-a8394108-9885-484a-a566-dc7ab2ef04a6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-a8394108-9885-484a-a566-dc7ab2ef04a6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                incorrect  \\\n",
       "94511   It is not more famous than Kyoto , but it has ...   \n",
       "370365    Seattle Mariners was against Losangels Angels .   \n",
       "365572  Because of you , I knew how to appreciate the ...   \n",
       "140865                    you ever to say with you self ?   \n",
       "76135   Japanese women soccer team won the World Cup g...   \n",
       "346202  The Internet helps us with communicatig with f...   \n",
       "371230  I thought it is certainly compared with the Sa...   \n",
       "318578  The dentist got rid out of the bad part on the...   \n",
       "86311                     First of all , boil soy beens .   \n",
       "161455     because , those hostels has a community room ,   \n",
       "291771  We take my dog and go out for a walk on the be...   \n",
       "403716  Hi everyone hope you have passed the great tim...   \n",
       "299214                                and weak up early .   \n",
       "23999   Fortunately , my junior 's magic went well , a...   \n",
       "248236  So if you are not satisfied with your current ...   \n",
       "24705   Thinking of that , smoking prohibition seems u...   \n",
       "84179   Thanks god it 's Saturday . Some websites for ...   \n",
       "3423    The practice is to translate the simple Japane...   \n",
       "19115      I rarely heard that people use ' one another '   \n",
       "4734                                 to read in English .   \n",
       "\n",
       "                                              english_inp  \\\n",
       "94511   <start> It is no more famous than Kyoto , but ...   \n",
       "370365  <start> Seattle Mariners were  went againstthe...   \n",
       "365572  <start> Because of you , I knew how to appreci...   \n",
       "140865              <start> Do you ever say to yourself ,   \n",
       "76135   <start> The Japanese women 's soccer team won ...   \n",
       "346202  <start> The Internet helps us with communicati...   \n",
       "371230  <start> I thought it is certainly comparable w...   \n",
       "318578  <start> The dentist got rid of the bad part of...   \n",
       "86311             <start> First of all , boil soy beans .   \n",
       "161455  <start> because those hostels have a community...   \n",
       "291771  <start> We took my dog and went out for a walk...   \n",
       "403716  <start> Hi everyone , Hope you had a great tim...   \n",
       "299214               <start> instead of waking up early .   \n",
       "23999   <start> Fortunately , my junior 's magic went ...   \n",
       "248236  <start> So if you are not satisfied with your ...   \n",
       "24705   <start> Keeping that in mind , a ban on smokin...   \n",
       "84179   <start> Thank God it 's Saturday . Some websit...   \n",
       "3423    <start> The practice is to translate a simple ...   \n",
       "19115   <start> I do n't hear people use  ' one anothe...   \n",
       "4734                      <start> to read it in English .   \n",
       "\n",
       "                                              english_out  \\\n",
       "94511   It is no more famous than Kyoto , but it has a...   \n",
       "370365  Seattle Mariners were  went againsttheLos Ange...   \n",
       "365572  Because of you , I knew how to appreciate the ...   \n",
       "140865                Do you ever say to yourself , <end>   \n",
       "76135   The Japanese women 's soccer team won the Worl...   \n",
       "346202  The Internet helps us with communicating with ...   \n",
       "371230  I thought it is certainly comparable with the ...   \n",
       "318578  The dentist got rid of the bad part of the too...   \n",
       "86311               First of all , boil soy beans . <end>   \n",
       "161455  because those hostels have a community room , ...   \n",
       "291771  We took my dog and went out for a walk on the ...   \n",
       "403716  Hi everyone , Hope you had a great time with t...   \n",
       "299214                 instead of waking up early . <end>   \n",
       "23999   Fortunately , my junior 's magic went well , w...   \n",
       "248236  So if you are not satisfied with your current ...   \n",
       "24705   Keeping that in mind , a ban on smoking seems ...   \n",
       "84179   Thank God it 's Saturday . Some websites for r...   \n",
       "3423    The practice is to translate a simple Japanese...   \n",
       "19115   I do n't hear people use  ' one another ' very...   \n",
       "4734                        to read it in English . <end>   \n",
       "\n",
       "                                       PREDICTED_SENTENCE  \n",
       "94511   is is not more than but it it it it is a castl...  \n",
       "370365                               was was was against   \n",
       "365572  of of you know know know how to appreciate the...  \n",
       "140865                           you you ever say to you   \n",
       "76135   a a a 's team team won the game game game and ...  \n",
       "346202  helps helps helps us with communicating with f...  \n",
       "371230  certainly certainly it certainly certainly as ...  \n",
       "318578  dentist dentist got rid of the bad part of the...  \n",
       "86311                    all all all boil boil soy beans   \n",
       "161455       because those hostels have a community room   \n",
       "291771  take take my dog and walk out for a walk on th...  \n",
       "403716  everyone everyone i i i hope hope wish time i ...  \n",
       "299214                           and of getting up early   \n",
       "23999   my my my junior boy shooting was there and and...  \n",
       "248236  if if you are not satisfied with my current jo...  \n",
       "24705   that that there a a a ban a smoking may as and...  \n",
       "84179     god god god 's title title title title for the   \n",
       "3423    the the is to translate simple simple sentence...  \n",
       "19115   honestly honestly n't often koreans frequently...  \n",
       "4734                                             to read   "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train['PREDICTED_SENTENCE']= result\n",
    "sample_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Gk2shbPIceM"
   },
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "01z2NYvpJFCO"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBE1SSvSJdxb",
    "outputId": "6760db99-5c52-473f-d398-9b50395fda6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people always orders water  , but today nobody ordered it .\n",
      "they they always gather water but but the nobody ordered it \n",
      "BLEU score: 0.7071067811865476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = sample_train.incorrect.values[20]\n",
    "translate = sample_train.PREDICTED_SENTENCE.values[20]\n",
    "print(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(nltk.translate.bleu_score.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-w_sHG_HIbNt",
    "outputId": "f94914b6-02e9-4f2a-b504-7ea99a8350d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 277.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average BLEU score of these sentences. are : 0.737659553608704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "score = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    inp = sample_train.incorrect.values[i]\n",
    "\n",
    "    # print(inp)\n",
    "    translate = sample_train.PREDICTED_SENTENCE.values[i]\n",
    "    # print(translate)\n",
    "    bleu = nltk.translate.bleu_score.sentence_bleu(inp, translate)\n",
    "    score = score + bleu\n",
    "average_bleu = score / 1000\n",
    "print('the average BLEU score of these sentences. are :' , average_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwPLy29_IbJ7",
    "outputId": "4b14b909-054b-4636-8738-697b52d828b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_encoder_layer_call_fn, embedding_layer_encoder_layer_call_and_return_conditional_losses, embedding_layer_decoder_layer_call_fn, embedding_layer_decoder_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0b0016a4-8c4c-4b4c-8698-140533e2e359/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0b0016a4-8c4c-4b4c-8698-140533e2e359/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f6fbf2f2f90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f6fbec1f710> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "with open(\"encode.pickle\",\"wb\") as temp1:\n",
    "   pickle.dump(vanilla,temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "immediate-opinion"
   },
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcdUmYYRBbWt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KqLB3tQ1dxAe",
    "outputId": "6eb28801-13da-43eb-cc72-eed55b1dc93c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.8.2'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Conv2D, Flatten , Input , Conv1D , Concatenate , MaxPooling1D , Dropout , Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import datetime\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding\n",
    "from sklearn.metrics import  f1_score , roc_auc_score\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4KIsGxLOfK"
   },
   "source": [
    "### <font color='blue'>**Implement custom encoder decoder and attention layers**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMm3ADQDLOfK"
   },
   "source": [
    "<font color='blue'>**Encoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Lx_5NA24KzRp"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        self.inp_vocab_size = inp_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_length = input_length\n",
    "        self.lstm_size= lstm_size\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "        self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.lstm = LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        input_embedd = self.embedding(input_sequence)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd, states)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      self.lstm_state_h = tf.zeros([batch_size , self.lstm_size])\n",
    "      self.lstm_state_c = tf.zeros([batch_size , self.lstm_size])\n",
    "      return self.lstm_state_h,self.lstm_state_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXn278lhLYRM"
   },
   "source": [
    "<font color='blue'>**Attention**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ab5SNdPZLlur"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super(Attention, self).__init__()\n",
    "    self.scoring_function = scoring_function\n",
    "    self.att_units = att_units\n",
    "    self.W1 = tf.keras.layers.Dense(att_units)\n",
    "    self.W2 = tf.keras.layers.Dense(att_units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    \n",
    "    if self.scoring_function == 'dot':\n",
    "        decoder_hidden_state_reshaped = tf.reshape(decoder_hidden_state , (decoder_hidden_state.shape[0],decoder_hidden_state.shape[1],1))\n",
    "\n",
    "        #I WAS USING tf.keras.layers.Dot FOR DOT PRODUCT , BUT IT GAVE INCOMPATIBILITY IN SHAPES , SO NOW I VE USED tf.keras.layers.dot\n",
    "        score =  tf.keras.layers.dot([ encoder_output , decoder_hidden_state_reshaped] , [2,1]) \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * encoder_output  #\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "        pass\n",
    "    elif self.scoring_function == 'general':\n",
    "        decoder_hidden_state_reshaped = tf.reshape(decoder_hidden_state , (decoder_hidden_state.shape[0],decoder_hidden_state.shape[1],1))\n",
    "        W = tf.random.uniform(shape=[encoder_output.shape[0] , self.att_units , self.att_units])\n",
    "        score =  tf.keras.layers.dot([ encoder_output , W] , [2,1]) \n",
    "        score =  tf.keras.layers.dot([ score , decoder_hidden_state_reshaped] , [2,1]) \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * encoder_output  #\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "        pass\n",
    "    elif self.scoring_function == 'concat':\n",
    "\n",
    "        decoder_hidden_state_reshaped = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        score =  self.V(tf.nn.tanh(self.W1(decoder_hidden_state_reshaped) + self.W2(encoder_output)) )\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * encoder_output  #\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic-FNEbfL2DN"
   },
   "source": [
    "<font color='blue'>**OneStepDecoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Kc8m7lmOL097"
   },
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(One_Step_Decoder, self).__init__()\n",
    "\n",
    "        # Initialize decoder embedding layer, LSTM \n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "\n",
    "        self.attention=Attention(score_fun,att_units)\n",
    "        self.embedding = tf.keras.layers.Embedding(tar_vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.dec_units , return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
    "        self.dense = tf.keras.layers.Dense(self.tar_vocab_size)\n",
    "\n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        output = self.embedding(input_to_decoder) # (32, 1, 12)\n",
    "        context_vector,attention_weights=self.attention(state_h,encoder_output)\n",
    "        concat = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
    "        lstm_output, state_h, state_c = self.lstm(concat)\n",
    "        \n",
    "        output = self.dense(lstm_output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        return output,state_h,state_c,attention_weights,context_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FHrurjUMGAi"
   },
   "source": [
    "<font color='blue'>**Decoder**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "XwZ2t4tncnUr"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(Decoder , self).__init__()\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "\n",
    "        self.onestep_decoder=One_Step_Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "\n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        all_outputs = tf.TensorArray(tf.float32, size=tf.shape(input_to_decoder)[1])\n",
    "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
    "            output,state_h,state_c,attention_weights,context_vector = self.onestep_decoder(input_to_decoder[: , timestep : timestep + 1] , \\\n",
    "                                                                                           encoder_output , decoder_hidden_state , decoder_cell_state)\n",
    "\n",
    "\n",
    "            all_outputs = all_outputs.write(timestep , output)\n",
    "        all_outputs = tf.transpose(all_outputs.stack() , [1,0,2])\n",
    "        return all_outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3KY8oCFYD51",
    "outputId": "31332e76-fc7f-45c7-aa17-b09e8dba984a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 13)\n"
     ]
    }
   ],
   "source": [
    "out_vocab_size=13 \n",
    "embedding_dim=12 \n",
    "input_length=10\n",
    "dec_units=16 \n",
    "att_units=16\n",
    "batch_size=32\n",
    "\n",
    "target_sentences=tf.random.uniform(shape=(batch_size,input_length),maxval=10,minval=0,dtype=tf.int32)\n",
    "encoder_output=tf.random.uniform(shape=[batch_size,input_length,dec_units])\n",
    "state_h=tf.random.uniform(shape=[batch_size,dec_units])\n",
    "state_c=tf.random.uniform(shape=[batch_size,dec_units])\n",
    "score_fun = 'concat'\n",
    "decoder=Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "output=decoder(target_sentences,encoder_output, state_h, state_c)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC1T1EOoMTqC"
   },
   "source": [
    "<font color='blue'>**Encoder Decoder model**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "QnPvt-oBLNJT"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, max_len):\n",
    "        self.encoder_inps = data['incorrect'].values\n",
    "        self.decoder_inps = data['english_inp'].values\n",
    "        self.decoder_outs = data['english_out'].values\n",
    "        self.tknizer_CORRECT_SENTENCE = tknizer_CORRECT_SENTENCE\n",
    "        self.tknizer_ERRONEOUS_SENTENCE = tknizer_ERRONEOUS_SENTENCE\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_ERRONEOUS_SENTENCE.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "\n",
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        \n",
    "        return [batch[0],batch[1]],batch[2]\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tAlrQSNLNJV",
    "outputId": "e76d8b26-1e13-4ce8-d85c-6f2606cb5bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 20) (512, 20) (512, 20)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
    "\n",
    "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=512)\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "FfqBIe20MT3D"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self,score_fun , encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
    "        super().__init__()\n",
    "        #encoder decoder\n",
    "\n",
    "        self.score_fun = score_fun\n",
    "\n",
    "        self.encoder=Encoder(inp_vocab_size = vocab_size_ERRONEOUS_SENTENCE+1,embedding_size = 50,lstm_size = 64,input_length = encoder_inputs_length)\n",
    "        self.decoder=Decoder(out_vocab_size = vocab_size_CORRECT_SENTENCE+1, embedding_dim = 100, input_length = decoder_inputs_length, dec_units =  64 \\\n",
    "                             ,score_fun = self.score_fun ,att_units = 64)\n",
    "\n",
    "    def call(self,data):\n",
    "\n",
    "        input,output = data[0], data[1]\n",
    "        initial_state= self.encoder.initialize_states(batch_size)\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input , initial_state)\n",
    "\n",
    "        decoder_output= self.decoder(output,encoder_output, encoder_h, encoder_c)\n",
    "\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TV5Vr3yiNEpC",
    "outputId": "c143c143-fef4-4dae-a30f-fe4938cc76a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "320/320 [==============================] - 195s 595ms/step - loss: 4.2743 - val_loss: 3.9678\n",
      "Epoch 2/30\n",
      "320/320 [==============================] - 192s 599ms/step - loss: 3.9670 - val_loss: 3.9571\n",
      "Epoch 3/30\n",
      "320/320 [==============================] - 192s 599ms/step - loss: 3.8562 - val_loss: 3.7938\n",
      "Epoch 4/30\n",
      "320/320 [==============================] - 191s 596ms/step - loss: 3.8870 - val_loss: 3.8076\n",
      "Epoch 5/30\n",
      "320/320 [==============================] - 192s 599ms/step - loss: 3.7736 - val_loss: 3.9933\n",
      "Epoch 6/30\n",
      "320/320 [==============================] - 189s 592ms/step - loss: 3.8538 - val_loss: 3.8079\n",
      "Epoch 7/30\n",
      "320/320 [==============================] - 189s 590ms/step - loss: 3.8605 - val_loss: 3.7680\n",
      "Epoch 8/30\n",
      "320/320 [==============================] - 189s 591ms/step - loss: 3.8621 - val_loss: 3.9076\n",
      "Epoch 9/30\n",
      "320/320 [==============================] - 192s 598ms/step - loss: 3.8190 - val_loss: 3.8800\n",
      "Epoch 10/30\n",
      "320/320 [==============================] - 189s 590ms/step - loss: 3.8125 - val_loss: 3.8884\n",
      "Epoch 11/30\n",
      "320/320 [==============================] - 191s 597ms/step - loss: 3.7210 - val_loss: 3.7202\n",
      "Epoch 12/30\n",
      "320/320 [==============================] - 189s 591ms/step - loss: 3.7823 - val_loss: 3.7624\n",
      "Epoch 13/30\n",
      "320/320 [==============================] - 191s 597ms/step - loss: 3.6810 - val_loss: 3.6518\n",
      "Epoch 14/30\n",
      "320/320 [==============================] - 189s 590ms/step - loss: 3.6844 - val_loss: 3.6846\n",
      "Epoch 15/30\n",
      "320/320 [==============================] - 189s 591ms/step - loss: 3.6941 - val_loss: 3.6849\n",
      "Epoch 16/30\n",
      "320/320 [==============================] - 191s 597ms/step - loss: 3.7064 - val_loss: 3.8900\n",
      "Epoch 17/30\n",
      "320/320 [==============================] - 191s 596ms/step - loss: 3.7248 - val_loss: 3.6828\n",
      "Epoch 18/30\n",
      "320/320 [==============================] - 191s 596ms/step - loss: 3.7076 - val_loss: 3.8075\n",
      "Epoch 19/30\n",
      "320/320 [==============================] - 190s 595ms/step - loss: 3.6553 - val_loss: 3.6498\n",
      "Epoch 20/30\n",
      "320/320 [==============================] - 188s 587ms/step - loss: 3.6787 - val_loss: 3.6184\n",
      "Epoch 21/30\n",
      "320/320 [==============================] - 188s 587ms/step - loss: 3.6244 - val_loss: 3.6898\n",
      "Epoch 22/30\n",
      "320/320 [==============================] - 190s 593ms/step - loss: 3.7729 - val_loss: 3.8470\n",
      "Epoch 23/30\n",
      "320/320 [==============================] - 191s 595ms/step - loss: 3.6737 - val_loss: 3.7877\n",
      "Epoch 24/30\n",
      "320/320 [==============================] - 191s 595ms/step - loss: 3.6870 - val_loss: 3.6219\n",
      "Epoch 25/30\n",
      "320/320 [==============================] - 188s 589ms/step - loss: 3.6131 - val_loss: 3.6216\n",
      "Epoch 26/30\n",
      "320/320 [==============================] - 190s 595ms/step - loss: 3.5568 - val_loss: 3.5596\n",
      "Epoch 27/30\n",
      "320/320 [==============================] - 190s 595ms/step - loss: 3.6621 - val_loss: 3.7623\n",
      "Epoch 28/30\n",
      "320/320 [==============================] - 188s 586ms/step - loss: 3.6434 - val_loss: 3.6515\n",
      "Epoch 29/30\n",
      "320/320 [==============================] - 190s 595ms/step - loss: 3.5761 - val_loss: 3.7200\n",
      "Epoch 30/30\n",
      "320/320 [==============================] - 191s 597ms/step - loss: 3.6151 - val_loss: 3.5730\n",
      "Model: \"encoder_decoder_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  2980640   \n",
      "                                                                 \n",
      " decoder_4 (Decoder)         multiple                  7514479   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,495,119\n",
      "Trainable params: 10,495,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=512\n",
    "score_fun  = 'general'\n",
    "att_units = 64\n",
    "model_2  = encoder_decoder(score_fun = score_fun , encoder_inputs_length=20,decoder_inputs_length=10,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_2.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
    "train_steps=train.shape[0]//1024\n",
    "valid_steps=validation.shape[0]//1024\n",
    "#TRANING THE MODEL FOR 20 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
    "log_dir=\"logs1\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "model_2.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=30, validation_data=train_dataloader, validation_steps=valid_steps , callbacks = checkpoint)\n",
    "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "SwuT0ilOWyep"
   },
   "outputs": [],
   "source": [
    "os.mkdir('/content/drive/MyDrive/saved_model_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "9nBeay4BWxZm"
   },
   "outputs": [],
   "source": [
    "model_2.save_weights('/content/drive/MyDrive/saved_model_attention/attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fkn1yHlWHjJQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_mc_2MyWxSx",
    "outputId": "24d57cfa-96a5-4bba-dca8-d056c9136db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  2980640   \n",
      "                                                                 \n",
      " decoder_4 (Decoder)         multiple                  7514479   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,495,119\n",
      "Trainable params: 10,495,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.load_weights(\"/content/drive/MyDrive/saved_model_attention/attention.h5\")\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DpC9zlzMcXp"
   },
   "source": [
    "## <font color='blue'>**Inference**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1IhdBrgQYJr"
   },
   "source": [
    "<font color='blue'>**CORRECTING THE INCORRECT SENTENCES**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MP3kLZoPMvSu",
    "outputId": "f9ae84e1-8812-417b-94c8-1de9e8ad1b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to neighbors , the shop is very popular among people in Fukui .\n",
      "i 'm a friend\n",
      "BLEU score: 0.8857000285382948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "def predict(input_sentence):\n",
    "    pred = []\n",
    "    input_sequence = tknizer_CORRECT_SENTENCE.texts_to_sequences([input_sentence])\n",
    "    result = ' '\n",
    "    encoder_seq = pad_sequences(input_sequence, maxlen=20, dtype='int32', padding='post')  \n",
    "    initial_state = model_2.layers[0].initialize_states(1)\n",
    "    encoder_output, encoder_h, encoder_c = model_2.layers[0](tf.constant(encoder_seq), initial_state)\n",
    "    start_index = tf.constant([[tknizer_CORRECT_SENTENCE.word_index['<start>']]])\n",
    "    states = [encoder_h, encoder_c]\n",
    "\n",
    "    for i in range(20): \n",
    "        # print(start_index)\n",
    "        predicted_out,state_h,state_c,attention_weights,context_vector = model_2.layers[1].onestep_decoder(start_index, encoder_output , encoder_h, encoder_c )\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        start_index = np.reshape(np.argmax(predicted_out), (1, 1))\n",
    "        pred.append(tknizer_CORRECT_SENTENCE.index_word[start_index[0][0]])\n",
    "        if(pred[-1]=='<end>'):\n",
    "            break\n",
    "        translated_sentence = ' '.join(pred)\n",
    "    return translated_sentence\n",
    "\n",
    "inp = validation.values[2000][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OH25UaQH1DZ",
    "outputId": "80165e58-f9dd-4bc4-f766-48822376507f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because they study hard foriegn lungage for the various goal .\n",
      "i 'm a friend\n",
      "BLEU score: 0.8857000285382948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[2][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prdmzWvhH1Db",
    "outputId": "719a8b88-f5ad-4b0e-9a42-5547c3e6abf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Especially her listening comprehension skill was really good .\n",
      "i 'm a friend\n",
      "BLEU score: 0.8857000285382948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[20][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItU3DRZ_H1Dc",
    "outputId": "cb6be7ea-c8d8-4eae-b4e3-cd64284381a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to neighbors , the shop is very popular among people in Fukui .\n",
      "i 'm a friend\n",
      "BLEU score: 0.8857000285382948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[2000][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "s6KDp62yH1Dd"
   },
   "outputs": [],
   "source": [
    "#USING THIS METHOD TO GET THE BLEU SCORE , THE REFERENCE NOTEBOOK METHOD SHOWS ME AN ERROR FOR 1000 FILES , BUT IT WORKS FINE FOR SINGLE FILE\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnnOeacuH1De",
    "outputId": "d9e30f99-6337-4c31-83e2-c840a79bc9bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because they study hard foriegn lungage for the various goal .\n",
      "i 'm a friend\n",
      "BLEU score: 0.8857000285382948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "inp = validation.values[2][0]\n",
    "print(inp)\n",
    "translate = predict(inp)\n",
    "print(translate)\n",
    "print('BLEU score: {}'.format(nltk.translate.bleu_score.sentence_bleu(inp, translate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiFzVJGhXUr6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9F0i-eyyXUmV",
    "outputId": "411a16af-2840-4a51-a15c-602b7715cf23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very good\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm really like i 'm really\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really like i 'm really like i 'm really like i 'm really like i 'm really like\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a book\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm really like i 'm really like i 'm really like i 'm really like i 'm really like\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really was a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm really like i 'm really like i 'm really like i 'm really like i 'm really like\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm going to the oth\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really like i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really want to hear\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear the facebook\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the oth\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm never judicial never judicial never judicial never judicial never judicial never judicial never judicial never judicial never judicial\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really like i 'm really like i 'm really like i 'm really like i 'm really like\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit there\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm never judicial is a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm in the most of my friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really like i 'm really like i 'm really like i 'm really like i 'm really like\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " 'i was a book',\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm really like i 'm really like i 'm really like i 'm really like i 'm really like\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to the movie\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm in the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " 'i was a friend',\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really and i 'm really want to visit and i 'm really and i 'm really and i\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to do n't a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write\",\n",
       " \"i 'm really like i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to hear the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm very much\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to write a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm too\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really like to the facebook\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm never judicial never judicial never judicial never judicial never judicial never judicial never judicial never judicial never judicial\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit the facebook\",\n",
       " \"i 'm going to friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm really a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm really\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm going to my friend\",\n",
       " \"i 'm going to visit\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm a friend\",\n",
       " \"i 'm really\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train = train.sample(1000)\n",
    "result = []\n",
    "i=0\n",
    "for i, j , k in tqdm(sample_train.values):\n",
    "    # print(i , j , k)\n",
    "    pred = predict(i)\n",
    "    result.append(pred)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "id": "vpMHogB_XyUc",
    "outputId": "90c4d8c1-c9e1-4f31-a7ab-b58c3ed1d359"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d19caa8f-55f9-4ec3-97bf-2de19829f758\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "      <th>PREDICTED_SENTENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243953</th>\n",
       "      <td>He said three students in the class including ...</td>\n",
       "      <td>&lt;start&gt; He said three students in the class , ...</td>\n",
       "      <td>He said three students in the class , includin...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305521</th>\n",
       "      <td>I introduce my favourite things .</td>\n",
       "      <td>&lt;start&gt; I 'll now introduce my favourite things .</td>\n",
       "      <td>I 'll now introduce my favourite things . &lt;end&gt;</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220752</th>\n",
       "      <td>Last Sunday , I rented a movie called  Patch A...</td>\n",
       "      <td>&lt;start&gt; Last Sunday , I rented a movie tited  ...</td>\n",
       "      <td>Last Sunday , I rented a movie tited  Patch Ad...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281708</th>\n",
       "      <td>The other day , I went to the zoo .</td>\n",
       "      <td>&lt;start&gt; The other dayI went to the zoo .</td>\n",
       "      <td>The other dayI went to the zoo . &lt;end&gt;</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166615</th>\n",
       "      <td>After eating meals , we played games .</td>\n",
       "      <td>&lt;start&gt; After eating our meal , we played games .</td>\n",
       "      <td>After eating our meal , we played games . &lt;end&gt;</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290983</th>\n",
       "      <td>What caught my eye were fluffy scarfs and whit...</td>\n",
       "      <td>&lt;start&gt; What caught my eye were fluffy scarfs ...</td>\n",
       "      <td>What caught my eye were fluffy scarfs and whit...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250168</th>\n",
       "      <td>The environment that I was raise was a living ...</td>\n",
       "      <td>&lt;start&gt; The environment that I was raised in w...</td>\n",
       "      <td>The environment that I was raised in was a liv...</td>\n",
       "      <td>i 'm really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317320</th>\n",
       "      <td>This picture is of sea at the island .</td>\n",
       "      <td>&lt;start&gt; This is a picture of the sea from the ...</td>\n",
       "      <td>This is a picture of the sea from the island ....</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131786</th>\n",
       "      <td>But I did n't get anything '</td>\n",
       "      <td>&lt;start&gt; But I did n't get anything</td>\n",
       "      <td>But I did n't get anything  &lt;end&gt;</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366386</th>\n",
       "      <td>Therefore if you do your best , you can do eve...</td>\n",
       "      <td>&lt;start&gt; Therefore if you do your best , you ca...</td>\n",
       "      <td>Therefore if you do your best , you can do eve...</td>\n",
       "      <td>i 'm going to visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345672</th>\n",
       "      <td>Shengbing is the most famours societies in my ...</td>\n",
       "      <td>&lt;start&gt; Shengbing is the most famous societies...</td>\n",
       "      <td>Shengbing is the most famous societies in my c...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200514</th>\n",
       "      <td>Can anybody tell me , what does the expression...</td>\n",
       "      <td>&lt;start&gt; Can anybody tell me what the expressio...</td>\n",
       "      <td>Can anybody tell me what the expression  cat p...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100932</th>\n",
       "      <td>Yesterday , I felt chilly and headache at work...</td>\n",
       "      <td>&lt;start&gt; Yesterday , I felt chilly and had a he...</td>\n",
       "      <td>Yesterday , I felt chilly and had a headache a...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420115</th>\n",
       "      <td>thanks to my patience and warm jacket , I coul...</td>\n",
       "      <td>&lt;start&gt; Thanks to my patience and warm jacket ...</td>\n",
       "      <td>Thanks to my patience and warm jacket , I mana...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331762</th>\n",
       "      <td>I can write only with easy words . . .</td>\n",
       "      <td>&lt;start&gt; I can only write easy words . . .</td>\n",
       "      <td>I can only write easy words . . . &lt;end&gt;</td>\n",
       "      <td>i 'm really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149241</th>\n",
       "      <td>in my home .</td>\n",
       "      <td>&lt;start&gt; at home .</td>\n",
       "      <td>at home . &lt;end&gt;</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395521</th>\n",
       "      <td>We will die and stand along front of God after...</td>\n",
       "      <td>&lt;start&gt; We will die and stand in front of God ...</td>\n",
       "      <td>We will die and stand in front of God after al...</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306161</th>\n",
       "      <td>Also , childrens ' crimes are not becaue of co...</td>\n",
       "      <td>&lt;start&gt; Also , childrens ' crimes are not beca...</td>\n",
       "      <td>Also , childrens ' crimes are not because of c...</td>\n",
       "      <td>i 'm going to my friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198987</th>\n",
       "      <td>I 'm going to bring new one .</td>\n",
       "      <td>&lt;start&gt; I 'm going to bring a new one .</td>\n",
       "      <td>I 'm going to bring a new one . &lt;end&gt;</td>\n",
       "      <td>i 'm a friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352138</th>\n",
       "      <td>And I heard about some realistic circumstances...</td>\n",
       "      <td>&lt;start&gt; By the way , I have heard some faction...</td>\n",
       "      <td>By the way , I have heard some faction about t...</td>\n",
       "      <td>i 'm going to my friend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d19caa8f-55f9-4ec3-97bf-2de19829f758')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d19caa8f-55f9-4ec3-97bf-2de19829f758 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d19caa8f-55f9-4ec3-97bf-2de19829f758');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                incorrect  \\\n",
       "243953  He said three students in the class including ...   \n",
       "305521                  I introduce my favourite things .   \n",
       "220752  Last Sunday , I rented a movie called  Patch A...   \n",
       "281708                The other day , I went to the zoo .   \n",
       "166615             After eating meals , we played games .   \n",
       "290983  What caught my eye were fluffy scarfs and whit...   \n",
       "250168  The environment that I was raise was a living ...   \n",
       "317320             This picture is of sea at the island .   \n",
       "131786                       But I did n't get anything '   \n",
       "366386  Therefore if you do your best , you can do eve...   \n",
       "345672  Shengbing is the most famours societies in my ...   \n",
       "200514  Can anybody tell me , what does the expression...   \n",
       "100932  Yesterday , I felt chilly and headache at work...   \n",
       "420115  thanks to my patience and warm jacket , I coul...   \n",
       "331762             I can write only with easy words . . .   \n",
       "149241                                       in my home .   \n",
       "395521  We will die and stand along front of God after...   \n",
       "306161  Also , childrens ' crimes are not becaue of co...   \n",
       "198987                      I 'm going to bring new one .   \n",
       "352138  And I heard about some realistic circumstances...   \n",
       "\n",
       "                                              english_inp  \\\n",
       "243953  <start> He said three students in the class , ...   \n",
       "305521  <start> I 'll now introduce my favourite things .   \n",
       "220752  <start> Last Sunday , I rented a movie tited  ...   \n",
       "281708           <start> The other dayI went to the zoo .   \n",
       "166615  <start> After eating our meal , we played games .   \n",
       "290983  <start> What caught my eye were fluffy scarfs ...   \n",
       "250168  <start> The environment that I was raised in w...   \n",
       "317320  <start> This is a picture of the sea from the ...   \n",
       "131786                <start> But I did n't get anything    \n",
       "366386  <start> Therefore if you do your best , you ca...   \n",
       "345672  <start> Shengbing is the most famous societies...   \n",
       "200514  <start> Can anybody tell me what the expressio...   \n",
       "100932  <start> Yesterday , I felt chilly and had a he...   \n",
       "420115  <start> Thanks to my patience and warm jacket ...   \n",
       "331762          <start> I can only write easy words . . .   \n",
       "149241                                  <start> at home .   \n",
       "395521  <start> We will die and stand in front of God ...   \n",
       "306161  <start> Also , childrens ' crimes are not beca...   \n",
       "198987            <start> I 'm going to bring a new one .   \n",
       "352138  <start> By the way , I have heard some faction...   \n",
       "\n",
       "                                              english_out  \\\n",
       "243953  He said three students in the class , includin...   \n",
       "305521    I 'll now introduce my favourite things . <end>   \n",
       "220752  Last Sunday , I rented a movie tited  Patch Ad...   \n",
       "281708             The other dayI went to the zoo . <end>   \n",
       "166615    After eating our meal , we played games . <end>   \n",
       "290983  What caught my eye were fluffy scarfs and whit...   \n",
       "250168  The environment that I was raised in was a liv...   \n",
       "317320  This is a picture of the sea from the island ....   \n",
       "131786                  But I did n't get anything  <end>   \n",
       "366386  Therefore if you do your best , you can do eve...   \n",
       "345672  Shengbing is the most famous societies in my c...   \n",
       "200514  Can anybody tell me what the expression  cat p...   \n",
       "100932  Yesterday , I felt chilly and had a headache a...   \n",
       "420115  Thanks to my patience and warm jacket , I mana...   \n",
       "331762            I can only write easy words . . . <end>   \n",
       "149241                                    at home . <end>   \n",
       "395521  We will die and stand in front of God after al...   \n",
       "306161  Also , childrens ' crimes are not because of c...   \n",
       "198987              I 'm going to bring a new one . <end>   \n",
       "352138  By the way , I have heard some faction about t...   \n",
       "\n",
       "             PREDICTED_SENTENCE  \n",
       "243953            i 'm a friend  \n",
       "305521            i 'm a friend  \n",
       "220752            i 'm a friend  \n",
       "281708            i 'm a friend  \n",
       "166615            i 'm a friend  \n",
       "290983            i 'm a friend  \n",
       "250168              i 'm really  \n",
       "317320            i 'm a friend  \n",
       "131786            i 'm a friend  \n",
       "366386      i 'm going to visit  \n",
       "345672            i 'm a friend  \n",
       "200514            i 'm a friend  \n",
       "100932            i 'm a friend  \n",
       "420115            i 'm a friend  \n",
       "331762              i 'm really  \n",
       "149241            i 'm a friend  \n",
       "395521            i 'm a friend  \n",
       "306161  i 'm going to my friend  \n",
       "198987            i 'm a friend  \n",
       "352138  i 'm going to my friend  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train['PREDICTED_SENTENCE']= result\n",
    "sample_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmxIVOOQPWMu"
   },
   "source": [
    "<font color='blue'>**Calculate BLEU score**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iHiLdROM23l",
    "outputId": "55f24ab1-2df0-4a99-d709-7f66a4cb0f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0\n"
     ]
    }
   ],
   "source": [
    "#Create an object of your custom model.\n",
    "#Compile and train your model on dot scoring function.\n",
    "# Visualize few sentences randomly in Test data\n",
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "\n",
    "#Sample example\n",
    "import nltk.translate.bleu_score as bleu\n",
    "reference = ['i am groot'.split(),] # the original\n",
    "translation = 'it is ship'.split() # trasilated using model\n",
    "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5Y6JrIxIao5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "1GRAMMAR_ERROR_HANDLING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
